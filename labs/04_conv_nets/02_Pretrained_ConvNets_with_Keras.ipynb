{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's make sure that we have access to a subset of image files from the PASCAL VOC dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "from zipfile import ZipFile\n",
    "\n",
    "if not op.exists(\"images_resize\"):\n",
    "    print('Extracting image files...')\n",
    "    zf = ZipFile('images_pascalVOC.zip')\n",
    "    zf.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pretrained model\n",
    "\n",
    "Objectives:\n",
    "\n",
    "- Load a pre-trained ResNet50 pre-trained model using Keras Zoo\n",
    "- Build a headless model and compute representations of images\n",
    "- Explore the quality of representation with t-SNE\n",
    "- Retrain last layer on cat vs dog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "\n",
    "model = ResNet50(include_top=True, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of an image\n",
    "\n",
    "**Exercise**\n",
    "- Open an image, preprocess it and build a batch of 1 image\n",
    "- Use the model to classify this image\n",
    "- Decode the predictions using `decode_predictions` from Keras\n",
    "\n",
    "Notes:\n",
    "- Test your code with `\"images_resize/000007.jpg\"`\n",
    "- You may need `preprocess_input` for preprocessing the image. \n",
    "- The Keras resnet expects floating point images of size `(224, 224)` with a dynamic in `[0, 255]` before preprocessing. [skimage's resize](http://scikit-image.org/docs/stable/api/skimage.transform.html#skimage.transform.resize) has a `preserve_range` flag that you might find useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "path = \"images_resize/000007.jpg\"\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/predict_image.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the representation of an image\n",
    "\n",
    "Let's build a new model that maps the image input space to the output of the layer before the last layer of the pretrained Resnet 50 model. We call this new model the \"base model\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = model.layers[0].input\n",
    "output = model.layers[-2].output\n",
    "base_model = Model(input, output)\n",
    "base_model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model can transform any image into a flat, high dimensional, semantic feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation = base_model.predict(img_batch)\n",
    "print(\"Shape of representation:\", representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Proportion of zeros in the feature vector: %0.3f\"\n",
    "      % np.mean(representation[0] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing representations of all images can be time consuming.\n",
    "This is usually made by large batches on a GPU for massive performance gains.\n",
    "\n",
    "For the remaining part, we will use pre-computed representations saved in h5 format.\n",
    "\n",
    "For those interested, this is done using the `process_images.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "paths = [\"images_resize/\" + path\n",
    "         for path in sorted(os.listdir(\"images_resize/\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('img_emb.h5', 'r') as h5f:\n",
    "    out_tensors = h5f['img_emb'][:]\n",
    "    \n",
    "out_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tensors.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- What is the proportion of 0 values in this representation?\n",
    "- Can you find any negative values?\n",
    "- Why are there so many zero values?\n",
    "- Are the zero always located in the same dimensions for different input images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/representations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find a 2D representation of that high dimensional feature space using T-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "img_emb_tsne = TSNE(perplexity=30).fit_transform(out_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(img_emb_tsne[:, 0], img_emb_tsne[:, 1]);\n",
    "plt.xticks(()); plt.yticks(());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add thumnails of the original images at their TSNE locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "def imscatter(x, y, paths, ax=None, zoom=1, linewidth=0):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    for x0, y0, p in zip(x, y, paths):\n",
    "        try:\n",
    "            im = imread(p)\n",
    "        except:\n",
    "            print(p)\n",
    "            continue\n",
    "        im = resize(im, (224, 224), preserve_range=False, mode='reflect')\n",
    "        im = OffsetImage(im, zoom=zoom)\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data',\n",
    "                            frameon=True, pad=0.1, \n",
    "                            bboxprops=dict(edgecolor='red',\n",
    "                                           linewidth=linewidth))\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(50, 50))\n",
    "imscatter(img_emb_tsne[:, 0], img_emb_tsne[:, 1], paths, zoom=0.5, ax=ax)\n",
    "plt.savefig('tsne.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Search: finding similar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(img):\n",
    "    plt.figure()\n",
    "    img = imread(img)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 57\n",
    "\n",
    "def most_similar(idx, top_n=5):\n",
    "    dists = np.linalg.norm(out_tensors - out_tensors[idx], axis = 1)\n",
    "    sorted_dists = np.argsort(dists)\n",
    "    return sorted_dists[:top_n]\n",
    "\n",
    "sim = most_similar(idx)\n",
    "[display(paths[s]) for s in sim];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification from Nearest Neighbors?\n",
    "\n",
    "Using these representations, it may be possible to build a nearest neighbor classifier. However, the representations are learnt on ImageNet, which are centered images, when we input images from PascalVOC, more plausible inputs of a real world system.\n",
    "\n",
    "The next section explores this possibility by computing the histogram of similarities between one image and the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_norms = np.linalg.norm(out_tensors, axis=1, keepdims=True)\n",
    "normed_out_tensors = out_tensors / out_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_idx = 208\n",
    "dists_to_item = np.linalg.norm(out_tensors - out_tensors[item_idx],\n",
    "                               axis=1)\n",
    "cos_to_item = np.dot(normed_out_tensors, normed_out_tensors[item_idx]) \n",
    "plt.hist(cos_to_item, bins=30)\n",
    "display(paths[item_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately there is no clear separation of class boundaries visible in the histogram of similarities alone. We need some supervision to be able to classify images.\n",
    "\n",
    "With a labeled dataset, even with very little labels per class, one would be able to:\n",
    "- build a k-Nearest Neighbor model,\n",
    "- build a classification model such as a SVM.\n",
    "\n",
    "These approximate classifiers are useful in practice.\n",
    "See the `cat vs dog` home assignment with GPU for another example of this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = np.where(cos_to_item > 0.5)\n",
    "print(items)\n",
    "[display(paths[s]) for s in items[0]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
