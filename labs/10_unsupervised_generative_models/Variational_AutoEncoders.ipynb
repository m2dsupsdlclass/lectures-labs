{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Auto  Encoders\n",
    "\n",
    "- Reference: Adapted from the Keras example\n",
    "- Auto-Encoding Variational Bayes\n",
    "   https://arxiv.org/abs/1312.6114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.7\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Conv2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(0, 18):\n",
    "    plt.subplot(3, 6, i + 1)\n",
    "    plt.imshow(x_train[i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard full-connected VAE model\n",
    "\n",
    "Let's define a VAE model with fully connected MLPs for the encoder and decoder networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_standard = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test_standard = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "x_train_standard.shape, x_test_standard.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "latent_dim = 2\n",
    "intermediate_dim = 256\n",
    "\n",
    "\n",
    "def make_encoder(original_dim, intermediate_dim, latent_dim):\n",
    "    x = Input(shape=(original_dim,))\n",
    "    hidden = Dense(intermediate_dim, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(hidden)\n",
    "    z_log_var = Dense(latent_dim)(hidden)\n",
    "    return Model(inputs=x, outputs=[z_mean, z_log_var],\n",
    "                name=\"mlp_encoder\")\n",
    "\n",
    "    \n",
    "encoder = make_encoder(original_dim, intermediate_dim, latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The VAE stochastic latent variable\n",
    "\n",
    "<img src=\"../../slides/10_unsupervised_generative_models/images/vae_3.svg\" width=\"600px\" />\n",
    "\n",
    "We use the reparametrization trick to define a random variable z that is conditioned on the input image x as follows:\n",
    "\n",
    "$$ z \\sim \\mathcal{N}(\\mu_z(x), \\sigma_z(x)) $$\n",
    "\n",
    "The reparametrization tricks defines $z$ has follows:\n",
    "\n",
    "$$ z = \\mu_z(x) + \\sigma_z(x) \\cdot \\epsilon$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0, 1) $$\n",
    "\n",
    "This way the dependency to between $z$ and $x$ is deterministic and differentiable. The randomness of $z$ only stems from $\\epsilon$ only for a given $x$.\n",
    "\n",
    "Note that in practice the output of the encoder network parameterizes $log(\\sigma^2_z(x)$ instead of $\\sigma_z(x)$. Taking the exponential of $log(\\sigma^2_z(x)$ ensures the positivity of the standard deviation from the raw output of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_func(inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),\n",
    "                              mean=0., stddev=1.)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "\n",
    "sampling_layer = Lambda(sampling_func, output_shape=(latent_dim,),\n",
    "                        name=\"latent_sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_decoder(latent_dim, intermediate_dim, original_dim):\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    x = Dense(intermediate_dim, activation='relu')(decoder_input)\n",
    "    x = Dense(original_dim, activation='sigmoid')(x)\n",
    "    return Model(decoder_input, x, name=\"mlp_decoder\")\n",
    "\n",
    "\n",
    "decoder = make_decoder(latent_dim, intermediate_dim, original_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the decoder outputs has random weights and output noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_z_from_prior = np.random.normal(loc=0, scale=1, size=(1, latent_dim))\n",
    "generated = decoder.predict(random_z_from_prior)\n",
    "plt.imshow(generated.reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated image is completely univariate noise: there is no apparent spatial depenedencies between the pixel values. This reflects the lack of prior structure in the randomly initialized fully-connected decoder network. \n",
    "\n",
    "\n",
    "Let's now the plug the encoder and decoder via the stochastic latent variable $z$ to get the full VAE architecture. The loss function is the negative ELBO of the variational inference problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vae(input_shape, encoder, decoder, sampling_layer):\n",
    "    # Build de model architecture by assembling the encoder,\n",
    "    # stochastic latent variable and decoder:\n",
    "    x = Input(shape=input_shape, name=\"input\")\n",
    "    z_mean, z_log_var = encoder(x)\n",
    "    z = sampling_layer([z_mean, z_log_var])\n",
    "    x_decoded_mean = decoder(z)\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "\n",
    "    # Define the VAE loss\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(\n",
    "        K.flatten(x), K.flatten(x_decoded_mean))\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    return vae\n",
    "\n",
    "vae = make_vae((original_dim,), encoder, decoder,\n",
    "               sampling_layer=sampling_layer)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vae.fit(x_train_standard, epochs=50, batch_size=100,\n",
    "        validation_data=(x_test_standard, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.save_weights(\"standard_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights(\"standard_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model has not yet converged even after 50 epochs. Furthermore it's is not overfitting significantly either. We chose a very low value for the latent dimension. It is likely that using the higher dimensional space could lead to a model either to optimize that would better fit the training set.\n",
    "\n",
    "By sampling a random latent vector from the prior distribution and feeding it to the decoder we can effectively sample from the image model trained by the VAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_z_from_prior = np.random.normal(size=(1, latent_dim))\n",
    "generated = decoder.predict(random_z_from_prior)\n",
    "plt.imshow(generated.reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `Ctrl-Enter` several times to sample from various random locations in the 2D latent space.\n",
    "\n",
    "The generated pictures are blurry but capture of the global organization of pixels required to represent samples from the 10 fashion item categories. The spatial structure has been learned and is only present in the decoder weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D plot of the image classes in the latent space\n",
    "\n",
    "We can also use the encoder to set the visualize the distribution of the test set in the 2D latent space of the VAE model. In the following the colors show the true class labels from the test samples.\n",
    "\n",
    "Note that the VAE is an unsupervised model: it did not use any label information during training. However we can observe that the 2D latent space is largely structured around the categories of images used in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_labels = {0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", \n",
    "                5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test_encoded, x_test_encoded_log_var = encoder.predict(x_test_standard, batch_size=100)\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test,\n",
    "            cmap=plt.cm.tab10)\n",
    "cb = plt.colorbar()\n",
    "cb.set_ticks(list(id_to_labels.keys()))\n",
    "cb.set_ticklabels(list(id_to_labels.values()))\n",
    "cb.update_ticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "- One can see that the class labels 5, 7 and 9 are grouped in a cluster of the latent space. Use matplotlib to display some samples from each of those 3 classes and discover why they have been grouped together by the VAE model.\n",
    "\n",
    "- Similarly: can you qualitatively explain with matplotlib why class 0, 4 and 6 seem to be hard to disentangle in this 2D latent space discovered by the VAE model?\n",
    "\n",
    "- One can observe that the global 2D shape of the encoded dataset is approximately spherical with values with a maximum radius of size 3. Where can you explain where the shape of this marginal latent distribution come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/class_5_7_9.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/class_0_4_6.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/shape_marginal_latent_distribution.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D panel view of samples from the VAE manifold\n",
    "\n",
    "The following linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian to produce values of the latent variables z. This makes it possible to use a square arangement of panels that spans the gaussian prior of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15  # figure with 15x15 panels\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection\n",
    "\n",
    "Let's rebuild a new VAE which encodes 9 of the 10 classes, and see if we can build a measure that shows wether the data is an anomaly\n",
    "We'll call standard classes the first 9 classes, and anomalies the last class (class n°9, which is \"sneakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indexes_train = y_train != 9\n",
    "valid_indexes_test = y_test != 9\n",
    "x_train_9 = x_train[valid_indexes]\n",
    "x_test_9 = x_test[valid_indexes_test]\n",
    "x_train_standard_9 = x_train_9.reshape((len(x_train_9), np.prod(x_train_9.shape[1:])))\n",
    "x_test_standard_9 = x_test_9.reshape((len(x_test_9), np.prod(x_test_9.shape[1:])))\n",
    "print(x_train_standard_9.shape, x_test_standard_9.shape)\n",
    "anomalies_indexes = y_test == 9\n",
    "anomalies = x_test_standard[anomalies_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild a new encoder, decoder, and train them on the limited dataset\n",
    "encoder = make_encoder(original_dim, intermediate_dim, latent_dim)\n",
    "decoder = make_decoder(latent_dim, intermediate_dim, original_dim)\n",
    "\n",
    "vae_9 = make_vae((original_dim,), encoder, decoder,\n",
    "               sampling_layer=sampling_layer)\n",
    "vae_9.fit(x_train_standard_9, epochs=50, batch_size=100,\n",
    "        validation_data=(x_test_standard_9, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we will do our sampling with numpy not with Keras or tensorflow\n",
    "def sampling_func_numpy(inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch_size = np.shape(z_mean)[0]\n",
    "    epsilon = np.random.normal(size=(batch_size, latent_dim),\n",
    "                              loc=0., scale=1.)\n",
    "    return z_mean + np.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "# Compute the reconstruction error: encode, sample, then decode. \n",
    "# To ensure we get a stable result, we'll run the sampling nb_sampling times\n",
    "def compute_reconstruction_error(img, nb_sampling=10):\n",
    "    if len(img.shape)==1:\n",
    "        img = np.expand_dims(img, 0)\n",
    "    batch_size = np.shape(img)[0]\n",
    "    img_encoded_mean_and_var = encoder.predict(img, batch_size=batch_size)\n",
    "    img_encoded_samples = [sampling_func_numpy(img_encoded_mean_and_var) for x in range(nb_sampling)]\n",
    "    # stack all samples\n",
    "    img_encoded_samples = np.vstack(img_encoded_samples)\n",
    "    reconstructed_samples = decoder.predict(img_encoded_samples)\n",
    "    # unstack all samples\n",
    "    split_samples = reconstructed_samples.reshape(nb_sampling, batch_size, img.shape[-1])\n",
    "    errors = np.linalg.norm(split_samples - img, axis=-1)\n",
    "    return np.mean(errors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_test = compute_reconstruction_error(x_test_standard_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_anomalies = compute_reconstruction_error(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_random = compute_reconstruction_error(np.random.uniform(size=(1000, 784),low=0.0, high=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most anomalous in test set\n",
    "indexes = np.argsort(errors_test)[-18:]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(0, 18):\n",
    "    plt.subplot(3, 6, i + 1)\n",
    "    plt.imshow(x_test_9[indexes][i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# It shows weird shaped tops, or very complex shoes which are difficult to reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most normal in anomalies test set \n",
    "indexes = np.argsort(errors_anomalies)[0:18]\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "for i in range(0, 18):\n",
    "    plt.subplot(3, 6, i + 1)\n",
    "    plt.imshow(x_test[anomalies_indexes][indexes][i], cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Indeed most of them do not look like sneakers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is this method a good anomaly detection method?\n",
    "\n",
    "Let's compare the distribution of reconstruction errors from \n",
    "- standard test set images\n",
    "- class 9 images\n",
    "- random noise\n",
    "\n",
    "What can you interpret from this graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "a1 = ax.hist(np.random.choice(errors_test, 1000, replace=False).tolist(), color=\"blue\", alpha=0.5,)\n",
    "a2 = ax.hist(errors_anomalies.tolist(), color=\"red\", alpha=0.5)\n",
    "a3 = ax.hist(errors_random.tolist(), color=\"green\", alpha=0.5)\n",
    "plt.legend(('standard', 'other class', 'random'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_conv = np.expand_dims(x_train, -1)\n",
    "x_test_conv = np.expand_dims(x_test, -1)\n",
    "x_train_conv.shape, x_test_conv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: write an encoder that uses a series of convolutional layers, with maxpooling or strided convolutions and Batch norm to encode the 2D, gray-level images into 2D latent vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "\n",
    "img_rows, img_cols, img_chns = 28, 28, 1\n",
    "filters = 32\n",
    "kernel_size = 3\n",
    "intermediate_dim = 128\n",
    "latent_dim = 2\n",
    "\n",
    "\n",
    "def make_conv_encoder(img_rows, img_cols, img_chns,\n",
    "                      latent_dim, intermediate_dim):\n",
    "    inp = x = Input(shape=(img_rows, img_cols, img_chns))\n",
    "    # TODO: write me!\n",
    "    return Model(inputs=inp, outputs=[z_mean, z_log_var],\n",
    "                 name='convolutional_encoder')\n",
    "\n",
    "\n",
    "conv_encoder = make_conv_encoder(img_rows, img_cols, img_chns,\n",
    "                                 latent_dim, intermediate_dim)\n",
    "print(conv_encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/conv_encoder.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stochastic latent variable is the same as for the fully-connected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_layer = Lambda(sampling_func, output_shape=(latent_dim,),\n",
    "                        name=\"latent_sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "The decoder is also convolutional but instead of downsampling the spatial dimensions from (28, 28) to 2 latent dimensions, it starts from the latent space to upsample a (28, 28) dimensions using strided `Conv2DTranspose` layers.\n",
    "\n",
    "Here again BatchNormalization layers are inserted after the convolution to make optimization converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv_decoder(latent_dim, intermediate_dim, original_dim,\n",
    "                      spatial_size=7, filters=16):\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    x = Dense(intermediate_dim, activation='relu')(decoder_input)\n",
    "    x = Dense(filters * spatial_size * spatial_size, activation='relu')(x)\n",
    "    x = Reshape((spatial_size, spatial_size, filters))(x)\n",
    "    # First up-sampling:\n",
    "    x = Conv2DTranspose(filters,\n",
    "                        kernel_size=3,\n",
    "                        padding='same',\n",
    "                        strides=(2, 2),\n",
    "                        activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2DTranspose(filters,\n",
    "                        kernel_size=3,\n",
    "                        padding='same',\n",
    "                        strides=1,\n",
    "                        activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # Second up-sampling:\n",
    "    x = Conv2DTranspose(filters,\n",
    "                        kernel_size=3,\n",
    "                        strides=(2, 2),\n",
    "                        padding='valid',\n",
    "                        activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # Ouput 1 channel of gray pixels values between 0 and 1:\n",
    "    x = Conv2D(1, kernel_size=2, padding='valid',\n",
    "               activation='sigmoid')(x)\n",
    "    return Model(decoder_input, x, name='convolutional_decoder')\n",
    "\n",
    "\n",
    "conv_decoder = make_conv_decoder(latent_dim, intermediate_dim, original_dim,\n",
    "                                 spatial_size=7, filters=filters)\n",
    "print(conv_decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = conv_decoder.predict(np.random.normal(size=(1, latent_dim)))\n",
    "plt.imshow(generated.reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new decoder encodes some a priori knowledge on the local dependencies between pixel values in the \"deconv\" architectures. Depending on the randomly initialized weights, the generated images can show some local spatial structure.\n",
    "\n",
    "Try to re-execute the above two cells several times to try to see the kind of local structure that stem from the \"deconv\" architecture it-self for different random initializations of the weights.\n",
    "\n",
    "\n",
    "Again, let's now plug everything to together to get convolutional version of a full VAE model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, img_chns)\n",
    "vae = make_vae(input_shape, conv_encoder, conv_decoder,\n",
    "               sampling_layer)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(x_train_conv, epochs=15, batch_size=100,\n",
    "        validation_data=(x_test_conv, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.save_weights(\"convolutional_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.load_weights(\"convolutional_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = conv_decoder.predict(np.random.normal(size=(1, latent_dim)))\n",
    "plt.imshow(generated.reshape(28, 28), cmap=plt.cm.gray)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D plot of the image classes in the latent space\n",
    "\n",
    "We find again a similar organization of the latent space. Compared to the fully-connected VAE space, the differnt class labels seem slightly better separated. This could be a consequence of the slightly better fit we obtain from the convolutional models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test_encoded, _ = conv_encoder.predict(x_test_conv, batch_size=100)\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test,\n",
    "            cmap=plt.cm.tab10)\n",
    "cb = plt.colorbar()\n",
    "cb.set_ticks(list(id_to_labels.keys()))\n",
    "cb.set_ticklabels(list(id_to_labels.values()))\n",
    "cb.update_ticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D panel view of samples from the VAE manifold\n",
    "\n",
    "The following linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian to produce values of the latent variables z. This makes it possible to use a square arangement of panels that spans the gaussian prior of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15  # figure with 15x15 panels\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * n, digit_size * n))\n",
    "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "for i, yi in enumerate(grid_x):\n",
    "    for j, xi in enumerate(grid_y):\n",
    "        z_sample = np.array([[xi, yi]])\n",
    "        x_decoded = conv_decoder.predict(z_sample)\n",
    "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "        figure[i * digit_size: (i + 1) * digit_size,\n",
    "               j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-supervised learning\n",
    "\n",
    "Let's reuse our encoder trained on many unlabeled samples to design a supervised model that can only use supervision from a small subset of samples with labels.\n",
    "\n",
    "To keep things simple we will just build a small supervised model on top of the latent representation defined by our encoder.\n",
    "\n",
    "We assume that we only have access to a small labeled subset with 50 examples per class (instead of 5000 examples per class in the full Fashion MNIST training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "\n",
    "small_x_train = []\n",
    "small_y_train = []\n",
    "num_per_class = 50\n",
    "\n",
    "for c in range(10):\n",
    "    class_mask = np.where(y_train==c)[0]\n",
    "    idx = rng.choice(class_mask, size=num_per_class, replace=False)\n",
    "    small_x_train += [x_train_conv[idx]]\n",
    "    small_y_train += [c] * num_per_class\n",
    "\n",
    "small_x_train = np.vstack(small_x_train)\n",
    "small_y_train = np.array(small_y_train)\n",
    "\n",
    "# reshuffle our small dataset\n",
    "perm = rng.permutation(range(small_y_train.shape[0]))\n",
    "small_x_train = small_x_train[perm]\n",
    "small_y_train = small_y_train[perm]\n",
    "\n",
    "small_x_train.shape, small_y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "- Use `conv_encoder` to project `small_x_train` into the latent space;\n",
    "- Define a small supervised 10-class classification network and use `small_y_train` to train it;\n",
    "- What test accuracy can you reach? What is the chance level?\n",
    "- Suggest what could be changed to improve the quality of our classification on this small labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement me!\n",
    "# define `small_x_train_encoded` for in the input training data\n",
    "# define a model named `mdl` with its layers and its loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/small_classifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdl.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.fit(small_x_train_encoded, small_y_train, \n",
    "        epochs=30, validation_data=[x_test_encoded, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = mdl.predict(x_test_encoded).argmax(axis=-1)\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "class_names = [name for id, name in sorted(id_to_labels.items())]\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "- Retrain the conv/deconv VAE model with a latent dim of 30 instead of 2. Generating the 2D manifold panels plots is no longer possible. However this richer latent space should make it possible to reach a much better test likelihood bound and generate higher quality images.\n",
    "\n",
    "- Adapt the convolutional architecture to retrain the model on the labeled faces in the wild (LFW) dataset instead (GPU needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
