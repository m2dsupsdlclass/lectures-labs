{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning a pretrained BERT model on MRPC task\n",
    "\n",
    "WIP\n",
    "\n",
    "- [x] Test on Colab\n",
    "- [ ] Add exercises\n",
    "- [ ] Add references and explanations\n",
    "- [ ] Include original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets the Colab tf version to 2.x\n",
    "# %tensorflow_version only exists in Colab.\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig, glue_convert_examples_to_features, BertForSequenceClassification, glue_processors\n",
    "\n",
    "# script parameters\n",
    "BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = BATCH_SIZE * 2\n",
    "USE_XLA = False\n",
    "USE_AMP = False\n",
    "EPOCHS = 1\n",
    "\n",
    "TASK = \"mrpc\"\n",
    "\n",
    "num_labels = len(glue_processors[TASK]().get_labels())\n",
    "\n",
    "tf.config.optimizer.set_jit(USE_XLA)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})\n",
    "\n",
    "# Load tokenizer and model from pretrained model/vocabulary. Specify the number of labels to classify (2+: classification, 1: regression)\n",
    "config = BertConfig.from_pretrained(\"bert-base-cased\", num_labels=num_labels)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset via TensorFlow Datasets\n",
    "data, info = tensorflow_datasets.load('glue/mrpc', with_info=True)\n",
    "train_examples = info.splits['train'].num_examples\n",
    "\n",
    "# MNLI expects either validation_matched or validation_mismatched\n",
    "valid_examples = info.splits['validation'].num_examples\n",
    "\n",
    "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, 128, TASK)\n",
    "\n",
    "# MNLI expects either validation_matched or validation_mismatched\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, 128, TASK)\n",
    "train_dataset = train_dataset.shuffle(128).batch(BATCH_SIZE).repeat(-1)\n",
    "valid_dataset = valid_dataset.batch(EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', config=config)\n",
    "\n",
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule \n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "if USE_AMP:\n",
    "    # loss scaling is currently required when using mixed precision\n",
    "    opt = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, 'dynamic')\n",
    "\n",
    "\n",
    "if num_labels == 1:\n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    "else:\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=opt, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "train_steps = train_examples//BATCH_SIZE\n",
    "valid_steps = valid_examples//EVAL_BATCH_SIZE\n",
    "\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, steps_per_epoch=train_steps,\n",
    "                    validation_data=valid_dataset, validation_steps=valid_steps)\n",
    "\n",
    "# Save TF2 model\n",
    "os.makedirs('./save/', exist_ok=True)\n",
    "model.save_pretrained('./save/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
    "sentence_0 = 'This research was consistent with his findings.'\n",
    "sentence_1 = 'His findings were compatible with this research.'\n",
    "sentence_2 = 'His findings were not compatible with this research.'\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True)\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True)\n",
    "inputs_1 = {k:np.array([v]) for k,v in inputs_1.items()}\n",
    "inputs_2 = {k:np.array([v]) for k,v in inputs_2.items()}\n",
    "del inputs_1[\"special_tokens_mask\"]\n",
    "del inputs_2[\"special_tokens_mask\"]\n",
    "print(inputs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = model(inputs=inputs_1)[0].numpy().argmax().item()\n",
    "pred_2 = model(inputs=inputs_2)[0].numpy().argmax().item()\n",
    "print('sentence_1 is', 'a paraphrase' if pred_1 else 'not a paraphrase', 'of sentence_0')\n",
    "print('sentence_2 is', 'a paraphrase' if pred_2 else 'not a paraphrase', 'of sentence_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_3 = 'This is a totally irrelevant phrase'\n",
    "inputs_3 = tokenizer.encode_plus(sentence_0, sentence_3, add_special_tokens=True)\n",
    "inputs_3 = {k:np.array([v]) for k,v in inputs_3.items()}\n",
    "del inputs_3[\"special_tokens_mask\"]\n",
    "pred_3 = model(inputs=inputs_3)[0].numpy().argmax().item()\n",
    "print('sentence_3 is', 'a paraphrase' if pred_3 else 'not a paraphrase', 'of sentence_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
