{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using RetinaNet\n",
    "\n",
    "RetinaNet is a neural network architecture for object detection described in [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Doll√°r.\n",
    "\n",
    "The following shows how to use a [Keras based implementation](https://github.com/fizyr/keras-retinanet) along with model parameters pretrained on the [COCO object detection dataset](http://cocodataset.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q keras-retinanet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Pretrained Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"resnet50_coco_best_v2.1.0.h5\"\n",
    "BACKBONE_NAME = 'resnet50'\n",
    "PRETRAINED_BASE_URL = (\n",
    "    \"https://github.com/fizyr/keras-retinanet/\"\n",
    "    \"releases/download/0.5.1/\")\n",
    "\n",
    "if not os.path.exists(PRETRAINED_MODEL_NAME):\n",
    "    model_url = PRETRAINED_BASE_URL + PRETRAINED_MODEL_NAME\n",
    "    print(f\"Downloading {model_url}...\")\n",
    "    urlretrieve(model_url, PRETRAINED_MODEL_NAME)\n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -lh *.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet import models\n",
    "\n",
    "# load retinanet model\n",
    "model = models.load_model(PRETRAINED_MODEL_NAME, backbone_name=BACKBONE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Objects (Location and Classes) in Test Images\n",
    "\n",
    "We need to define a label to names mapping for visualization purposes: those labels match the classes from the COCO dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_names = {\n",
    "    0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane',\n",
    "    5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light',\n",
    "    10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench',\n",
    "    14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow',\n",
    "    20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack',\n",
    "    25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee',\n",
    "    30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite',\n",
    "    34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard',\n",
    "    37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass',\n",
    "    41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl',\n",
    "    46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli',\n",
    "    51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake',\n",
    "    56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed',\n",
    "    60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse',\n",
    "    65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave',\n",
    "    69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book',\n",
    "    74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear',\n",
    "    78: 'hair drier', 79: 'toothbrush'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def detect_and_visualize(image_bgr):\n",
    "    # copy to draw on\n",
    "    draw = image_bgr.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # preprocess image for network\n",
    "    image_input = image_bgr.copy()\n",
    "    image_input = preprocess_image(image_input)\n",
    "    image_input, scale = resize_image(image_input)\n",
    "    print(f\"shape: {image_input.shape}, dtype: {image_input.dtype}, \"\n",
    "          f\"range: {(image_input.min(), image.max())}\")\n",
    "\n",
    "    # process image\n",
    "    start = time.time()\n",
    "    boxes, scores, labels = model.predict_on_batch(\n",
    "        np.expand_dims(image_input, axis=0))\n",
    "    print(f\"processing time: {time.time() - start:.1f}s\")\n",
    "\n",
    "    # correct for image scale\n",
    "    boxes /= scale\n",
    "\n",
    "    # visualize detections\n",
    "    for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "        # scores are sorted so we can break\n",
    "        if score < 0.4:\n",
    "            break\n",
    "\n",
    "        color = label_color(label)\n",
    "\n",
    "        b = box.astype(int)\n",
    "        draw_box(draw, b, color=color)\n",
    "\n",
    "        caption = \"{} {:.3f}\".format(labels_to_names[label], score)\n",
    "        print(caption)\n",
    "        draw_caption(draw, b, caption)\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    \n",
    "\n",
    "# load image\n",
    "image = read_image_bgr('webcam_shot.jpeg')\n",
    "detect_and_visualize(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World Data\n",
    "\n",
    "Let's play with the laptop webcam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras_retinanet.utils.image import read_image_bgr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def camera_grab(camera_id=0, fallback_filename='webcam_shot.jpeg'):\n",
    "    camera = cv2.VideoCapture(camera_id)\n",
    "    try:\n",
    "        # take 10 consecutive snapshots to let the camera automatically tune\n",
    "        # itself and hope that the contrast and lightning of the last snapshot\n",
    "        # is good enough.\n",
    "        for i in range(10):\n",
    "            snapshot_ok, image = camera.read()\n",
    "        if not snapshot_ok:\n",
    "            print(\"WARNING: could not access camera\")\n",
    "            if fallback_filename:\n",
    "                image = read_image_bgr(fallback_filename)\n",
    "    finally:\n",
    "        camera.release()\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = camera_grab(camera_id=0)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_and_visualize(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
