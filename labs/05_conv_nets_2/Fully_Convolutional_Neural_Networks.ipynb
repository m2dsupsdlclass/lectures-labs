{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional Neural Networks\n",
    "\n",
    "Objectives:\n",
    "- Load a CNN model pre-trained on ImageNet\n",
    "- Transform the network into a Fully Convolutional Network \n",
    "- Apply the network perform weak segmentation on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from scipy.misc import imread as scipy_imread, imresize as scipy_imresize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper functions to disable annoying warnings:\n",
    "def imread(*args, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return scipy_imread(*args, **kwargs)\n",
    "\n",
    "\n",
    "def imresize(*args, **kwargs):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        return scipy_imresize(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained ResNet50\n",
    "# We use include_top = False for now,\n",
    "# as we'll import output Dense Layer later\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "base_model = ResNet50(include_top=False)\n",
    "\n",
    "print(base_model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res5c = base_model.layers[-1]\n",
    "type(res5c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res5c.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully convolutional ResNet\n",
    "\n",
    "- Out of the `res5c` residual block, the resnet outputs a tensor of shape $W \\times H \\times 2048$. \n",
    "- For the default ImageNet input, $224 \\times 224$, the output size is $7 \\times 7 \\times 2048$\n",
    "\n",
    "#### Regular ResNet layers \n",
    "\n",
    "The regular ResNet head after the base model is as follows: \n",
    "```py\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1000)(x)\n",
    "x = Softmax()(x)\n",
    "```\n",
    "\n",
    "Here is the full definition of the model: https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py/resnet50.py\n",
    "\n",
    "#### Our Version\n",
    "\n",
    "- We want to retrieve the labels information, which is stored in the Dense layer. We will load these weights afterwards\n",
    "- We will change the Dense Layer to a Convolution2D layer to keep spatial information, to output a $W \\times H \\times 1000$.\n",
    "- We can use a kernel size of (1, 1) for that new Convolution2D layer to pass the spatial organization of the previous layer unchanged (it's called a _pointwise convolution_).\n",
    "- We want to apply a softmax only on the last dimension so as to preserve the $W \\times H$ spatial information.\n",
    "\n",
    "#### A custom Softmax\n",
    "\n",
    "We build the following Custom Layer to apply a softmax only to the last dimension of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "# A custom layer in Keras must implement the four following methods:\n",
    "class SoftmaxMap(Layer):\n",
    "    # Init function\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(SoftmaxMap, self).__init__(**kwargs)\n",
    "\n",
    "    # There's no parameter, so we don't need this one\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "\n",
    "    # This is the layer we're interested in: \n",
    "    # very similar to the regular softmax but note the additional\n",
    "    # that we accept x.shape == (batch_size, w, h, n_classes)\n",
    "    # which is not the case in Keras by default.\n",
    "    # Note that we substract the logits by their maximum to\n",
    "    # make the softmax more numerically stable.\n",
    "    def call(self, x, mask=None):\n",
    "        e = K.exp(x - K.max(x, axis=self.axis, keepdims=True))\n",
    "        s = K.sum(e, axis=self.axis, keepdims=True)\n",
    "        return e / s\n",
    "\n",
    "    # The output shape is the same as the input shape\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we can use this layer to normalize the classes probabilities of some random spatial predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, w, h, n_classes = 10, 3, 4, 5\n",
    "random_data = np.random.randn(n_samples, w, h, n_classes)\n",
    "random_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because those predictions are random, if we some accross the classes dimensions we get random values instead of class probabilities that would need to some to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data[0].sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wrap the `SoftmaxMap` class into a test model to process our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential([SoftmaxMap(input_shape=(w, h, n_classes))])\n",
    "model.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_mapped_data = model.predict(random_data)\n",
    "softmax_mapped_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the values are now in the [0, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_mapped_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dimension now approximately sum to one, we can therefore be used as class probabilities (or parameters for a multinouli distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_mapped_data[0].sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the highest activated channel for each spatial location is still the same before and after the softmax map. The ranking of the activations is preserved as softmax is a monotonic function (when considered element-wise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data[0].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_mapped_data[0].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "- What is the shape of the convolution kernel we want to apply to replace the Dense ?\n",
    "- Build the fully convolutional model as described above. We want the output to preserve the spatial dimensions but output 1000 channels (one channel per class).\n",
    "- You may introspect the last elements of `base_model.layers` to find which layer to remove\n",
    "- You may use the Keras Convolution2D(output_channels, filter_w, filter_h) layer and our SotfmaxMap to normalize the result as per-class probabilities.\n",
    "- For now, ignore the weights of the new layer(s) (leave them initialized at random): just focus on making the right architecture with the right output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D\n",
    "from keras.models import Model\n",
    "\n",
    "input = base_model.layers[0].input\n",
    "\n",
    "# TODO: compute per-area class probabilites\n",
    "output = input\n",
    "\n",
    "fully_conv_ResNet = Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/fully_conv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following random data to check that it's possible to run a forward pass on a random RGB image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_maps = fully_conv_ResNet.predict(np.random.randn(1, 200, 300, 3))\n",
    "prediction_maps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you explain the resulting output shape?\n",
    "\n",
    "The class probabilities should sum to one in each area of the output map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_maps.sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dense weights\n",
    "\n",
    "- We provide the weights and bias of the last Dense layer of ResNet50 in file `weights_dense.h5`\n",
    "- Our last layer is now a 1x1 convolutional layer instead of a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('weights_dense.h5', 'r') as h5f:\n",
    "    w = h5f['w'][:]\n",
    "    b = h5f['b'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer = fully_conv_ResNet.layers[-2]\n",
    "\n",
    "print(\"Loaded weight shape:\", w.shape)\n",
    "print(\"Last conv layer weights shape:\", last_layer.get_weights()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the weights\n",
    "w_reshaped = w.reshape((1, 1, 2048, 1000))\n",
    "\n",
    "# set the conv layer weights\n",
    "last_layer.set_weights([w_reshaped, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A forward pass\n",
    "\n",
    "- We define the following function to test our new network. \n",
    "- It resizes the input to a given size, then uses `model.predict` to compute the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "\n",
    "def forward_pass_resize(img_path, img_size):\n",
    "    img_raw = imread(img_path)\n",
    "    print(\"Image shape before resizing: %s\" % (img_raw.shape,))\n",
    "    img = imresize(img_raw, size=img_size).astype(\"float32\")\n",
    "    img = preprocess_input(img[np.newaxis])\n",
    "    print(\"Image batch size shape before forward pass:\", img.shape)\n",
    "    z = fully_conv_ResNet.predict(img)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = forward_pass_resize(\"dog.jpg\", (800, 600))\n",
    "print(\"prediction map shape\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding dog-related classes\n",
    "ImageNet uses an ontology of concepts, from which classes are derived. A synset corresponds to a node in the ontology.\n",
    "\n",
    "For example all species of dogs are children of the synset [n02084071](http://image-net.org/synset?wnid=n02084071) (Dog, domestic dog, Canis familiaris):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper file for importing synsets from imagenet\n",
    "import imagenet_tool\n",
    "synset = \"n02084071\" # synset corresponding to dogs\n",
    "ids = imagenet_tool.synset_to_dfs_ids(synset)\n",
    "print(\"All dog classes ids (%d):\" % len(ids))\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dog_id in ids[:10]:\n",
    "    print(imagenet_tool.id_to_words(dog_id))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised heatmap of the class \"dog\"\n",
    "\n",
    "The following function builds a heatmap from a forward pass. It sums the representation for all ids corresponding to a synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_heatmap(z, synset):\n",
    "    class_ids = imagenet_tool.synset_to_dfs_ids(synset)\n",
    "    class_ids = np.array([id_ for id_ in class_ids if id_ is not None])\n",
    "    x = z[0, :, :, class_ids].sum(axis=0)\n",
    "    print(\"size of heatmap: \" + str(x.shape))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_img_and_heatmap(img_path, heatmap):\n",
    "    dog = imread(img_path)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(dog)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(heatmap, interpolation='nearest', cmap=\"viridis\")\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "- What is the size of the heatmap compared to the input image?\n",
    "- Build 3 dog heatmaps from `\"dog.jpg\"`, with the following sizes:\n",
    "  - `(400, 640)`\n",
    "  - `(800, 1280)`\n",
    "  - `(1600, 2560)`\n",
    "- What do you observe? \n",
    "\n",
    "You may plot a heatmap using the above function `display_img_and_heatmap`. You might also want to reuse `forward_pass_resize` to compute the class maps them-selves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dog synset\n",
    "s = \"n02084071\"\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/build_heatmaps.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the 3 heatmaps\n",
    "By combining the heatmaps at different scales, we obtain a much better information about the location of the dog.\n",
    "\n",
    "**Bonus**\n",
    "- Combine the three heatmap by resizing them to a similar shape, and averaging them\n",
    "- A geometric norm will work better than standard average!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/geom_avg.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus**\n",
    "\n",
    "Experiment with Semantic segmentation. You may train on COCO dataset http://mscoco.org/dataset/#overview\n",
    "\n",
    "- Use the GPU to precompute the activations of a headless and convolutionalized ResNet50 or Xception model;\n",
    "- Initialize the weights of a new Convolution2D(n_classes, 1, 1) at random;\n",
    "- Train the top of the segmentation model on class label data extracted from the MS COCO 2016 dataset;\n",
    "- Start with a single low resolution model. Then add multi-scale and see the improvement.\n",
    "\n",
    "To go further, consider open source implementation of models rather than building your own from scratch. For instance, FAIR's detection lib (in Caffe2) provides a lot of state of the art models. https://github.com/facebookresearch/Detectron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
