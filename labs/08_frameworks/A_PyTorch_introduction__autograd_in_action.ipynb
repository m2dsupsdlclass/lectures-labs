{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is a popular deep learning framework that was introduced around one year ago. It has a slightly lower-level API than Keras, but is much easier to use than TensorFlow when it comes to defining models with custom loss functions or exotic architectures with a structure that depends on the input data (e.g. for NLP). It is very popular among researchers and kagglers - a little less in the industry (yet).\n",
    "\n",
    "http://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles\n",
    "\n",
    "PyTorch essentially provides the user with two main utilities:\n",
    "\n",
    "- It allows to perform Tensor/vector computation on the GPU with an API similar (but not compatible) to Numpy.\n",
    "\n",
    "- It records all computation to be able to backpropagate through them. That is, provided a sequence of operations that starts from a tensor $\\theta$ to define a scalar $g(\\theta)$, it is able to compute \n",
    "$\\nabla_\\theta g(\\theta)$ exactly, with only one function call.\n",
    "\n",
    "Typically, $\\theta$ will be a parameter of a neural network and $g$ a loss function such as $\\ell(f_{\\theta}(x), y)$ for supervised learning. \n",
    "\n",
    "The essential difference with TensorFlow lies in the way $g$ is defined: in PyTorch, every node in the computation graph is defined on the fly while executing the forward pass, from within the Python interpreter: any Numpy code can be ported to PyTorch easily, and all flow control operation (e.g. loops, if/else, etc.) can be kept untouched. In contrast, TensorFlow requires by default the user to define a graph more declaratively. This graph is then used internally (i.e. outside the Python interpreter), to compute both the the predictions, the loss value and its derivatives.\n",
    "\n",
    "PyTorch takes care of recording everything it needs to do the backpropagation, *on the fly*.\n",
    "\n",
    "Note that recent versions of TensorFlow (1.5 and later) now come with the [eager mode](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md) to make TensorFlow use the define-by-run semantics of PyTorch (and Chainer). However this mode is quite new and not enabled by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "This set of notebooks **require PyTorch 0.4.0 or later**. You can check the version number of the currently installed pytorch package with: `import torch; print(torch.__version__)`.\n",
    "\n",
    "If you do not have a CUDA-based GPU on your machine please feel free to use the CPU-only version as those introductory notebooks do not require a GPU.\n",
    "\n",
    "Up-to-date installation instructions (including pip-based installation instructions) can be found on the official home page of the project: http://pytorch.org.\n",
    "\n",
    "If pytorch version is too old and was previously installed from another conda channel, conda upgrade might not work. You need to `conda remove pytorch` first and then reinstall it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the device\n",
    "PyTorch can run on either CPU or GPU. The new API (v0.4.0) lets us define it in a nice way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Two options: 'cuda' (for GPU) or 'cpu' (if no GPU available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Device type set:\", \"GPU\" if device.type == \"cuda\" else \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example\n",
    "\n",
    "We will define a vector $x \\in \\mathbb{R}^n$ and computes its norms using PyTorch. For this, we will define our first `Tensor` -- the Tensor object is the central element of PyTorch. It is very similar to a numpy array.\n",
    "\n",
    "Let us fill this vector with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "n = 3\n",
    "x = torch.rand(n, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` is a tensor, which can be manipulated roughly as a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a small API tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go back and forth from numpy to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = torch.from_numpy(np.ones((n, n)))\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all tensors are typed and that you can only do operations with tensors of the same type. Mixing tensor types will lead to a runtime error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    torch.matmul(A, x)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicit type conversion are required. Note while numpy uses the `dtype` attribute to store the type of the data items, this information is part of the type of the Tensor itself in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(A.float(), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python 3.5 and later it's possible to use the `@` operator syntax for matrix-matrix or matrix-vector multiplication operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.float() @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a norm function that returns the euclidean norm of a vector $x \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$f(x) = || x ||_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.sqrt(torch.sum(x ** 2, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `dim` keyword argument serves the same purpose as the `axis` keyword argument in `numpy.sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in computing $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$. \n",
    "\n",
    "\n",
    "**Exercise**:\n",
    "\n",
    "- Show that if $f(x) = || x ||_2$ then $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$.\n",
    "\n",
    "\n",
    "Assume we are too lazy to derive the analytical form of the gradient manually. Instead we will use the `autograd` facilities of PyTorch. Since PyTorch version 0.4.0, the Tensor classes can directly act as a variable to differentiate upon. Before that, we needed to wrap `Tensor` instances using the `Variable` class to get access to the `autograd` facilities. It is now made easier with the necessary methods included in the `Tensor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did we require autograd for our tensor x?\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_()\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the norm `f(x)` for our specific vector `x`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is a PyTorch scalar tensor (that also store the necessary provenance information to be able to compute a gradient). To convert into a regular Python scalar value we can use the `.item()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the gradient of this scalar variable with respect to all the tensor variables that were used to compute its value.\n",
    "\n",
    "The following `.backward()` call will assign `.grad` attributes to all `Tensor` variables used in the computation of $f$ and that are marked with the `requires_grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient $\\nabla_x f(x)$ can be found in `x.grad`, which is also a `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare it to the result of the evaluation of the analytical expression of the derivative of f(x) given x:  $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_grad = x / f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works ! You now know everything you need to know to use PyTorch. Note that, similar to Keras, PyTorch comes with a number of predefined functions that are useful in network definition. Check out http://pytorch.org/docs/ and tutorials for an overview of the tools you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "- Write a function the pytorch code for function `g` that computes the cosine similarity of two variable vectors with float entries $\\mathbf{x}$ and $\\mathbf{y}$:\n",
    "\n",
    "$$g(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x}^T \\mathbf{y}}{|| \\mathbf{x} ||_2 || \\mathbf{y} ||_2 }$$\n",
    "\n",
    "- Use `torch.autograd` to compute the derivatives with respect to $\\mathbf{x} \\in \\mathbb{R}^3$ and $\\mathbf{y} \\in \\mathbb{R}^3$ for some random values of your choice;\n",
    "\n",
    "- What is the expected value of the cosine similarity of two colinear vectors? What is the expected value of the gradients of the cosine function with respect to each of those input tensors?\n",
    "\n",
    "- Compute $\\nabla_x g(x, y)$ and $\\nabla_y g(x, y)$ for some choice of $\\mathbf{x} = \\alpha \\cdot \\mathbf{y}$ with any $\\alpha \\in \\mathbb{R}$. Check that you can get the expected result with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, y):\n",
    "    # TODO: fix the following code to implement the cosine\n",
    "    # similarity function instead.\n",
    "    return torch.sum(x) + torch.sum(y)\n",
    "\n",
    "\n",
    "x = torch.tensor([0, 1, 2], dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([3, 0.9, 2.2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "cosine = g(x, y)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cosine_autograd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cosine_autograd_colinear.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reinitialize our two variables to non colinear values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 1, 2], dtype=torch.float32, requires_grad=True)\n",
    "y = torch.tensor([3, 0.9, 2.2], dtype=torch.float32, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cells many times (use `ctrl-enter`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = g(x, y)\n",
    "print('cosine:', cosine)\n",
    "cosine.backward()\n",
    "with torch.no_grad():\n",
    "    # Do not record the following operations for future\n",
    "    # calls to cosine.backward().\n",
    "    x.add_(0.5 * x.grad)\n",
    "    y.add_(0.5 * y.grad)\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"x/y:\", x / y)\n",
    "print(\"x.grad:\", x.grad)\n",
    "print(\"y.grad:\", y.grad)\n",
    "x.grad.zero_()\n",
    "y.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradient descent methods\n",
    "\n",
    "In this notebook, we will use PyTorch to compare the different gradient methods and a toy 2D examples: we will try to find the minimum of the difference of two Gaussians. PyTorch provides a convenient wrapper named `nn.Module` to define parametrized functions, that we will use along this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class Norm(nn.Module):\n",
    "\n",
    "    def __init__(self, p=2.):\n",
    "        super(Norm, self).__init__()\n",
    "        self.p = Parameter(torch.tensor([p], dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        p_sum = torch.sum(torch.pow(x, self.p), dim=0)\n",
    "        return torch.pow(p_sum, 1 / self.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above definition, we treat the attribute `p` of the \"p-norm\" (a generalization of the euclidean norm) as a parameter that can be differentiated upon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.rand(n)\n",
    "norm = Norm(p=3.)\n",
    "v = norm(x)\n",
    "v.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access $\\nabla_p(x \\to || x ||_p)$ in `norm.p.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a Gaussian operator, along with a generic Gaussian combination. We will not consider the gradient w.r.t the parameters of these modules, hence we specify `requires_grad=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(nn.Module):\n",
    "\n",
    "    def __init__(self, precision, mean):\n",
    "        super(Gaussian, self).__init__()\n",
    "\n",
    "        assert precision.shape == (2, 2)\n",
    "        assert mean.shape == (2,)\n",
    "\n",
    "        self.precision = Parameter(precision, requires_grad=False)\n",
    "        self.mean = Parameter(mean, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute the (unormalized) likelihood of x given a Gaussian.\n",
    "        \n",
    "        x can be a 2D-vector or a batch of 2D-vectors.\n",
    "        \"\"\"\n",
    "        xc = x - self.mean\n",
    "        if len(xc.shape) == 1:\n",
    "            # Reshape xc to work as a mini-batch of one vector.\n",
    "            xc = xc.view(1, -1)\n",
    "        return torch.exp(-.5 * (torch.sum((xc @ self.precision) * xc, dim=1)))\n",
    "\n",
    "\n",
    "class GaussianCombination(nn.Module):\n",
    "\n",
    "    def __init__(self, precisions, means, weights):\n",
    "        super(GaussianCombination, self).__init__()\n",
    "        assert len(precisions) == len(means) == len(weights)\n",
    "        self.gaussians = nn.ModuleList()\n",
    "        for precision, mean in zip(precisions, means):\n",
    "            self.gaussians.append(Gaussian(precision, mean))\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        return sum(w * g(x) for g, w in zip(self.gaussians, self.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define $f(x) = \\exp(-(x- m_1)^T P_1 (x - m_1)) - \\exp(-(x- m_2)^T P_2 (x - m_2))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = torch.tensor([[1, 0], [0, 4]], dtype=torch.float32)\n",
    "m1 = torch.tensor([0, 1], dtype=torch.float)\n",
    "w1 = 1\n",
    "p2 = torch.tensor([[1, -2], [-2, 10]], dtype=torch.float32)\n",
    "m2 = torch.tensor([0, -2], dtype=torch.float32)\n",
    "w2 = -1\n",
    "\n",
    "f = GaussianCombination([p1, p2], [m1, m2], [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a plotting function to visualize $f$. Note the small boilerplate to interface PyTorch with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_function(f, ax):\n",
    "    x_max, y_max, x_min, y_min = 3, 3, -3, -3\n",
    "    x = np.linspace(x_min, x_max, 100, dtype=np.float32)\n",
    "    y = np.linspace(y_min, y_max, 100, dtype=np.float32)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    samples = np.concatenate((X[:, :, None], Y[:, :, None]), axis=2)\n",
    "    samples = samples.reshape(-1, 2)\n",
    "    samples = torch.from_numpy(samples).requires_grad_()\n",
    "    Z = f(samples).data.numpy()\n",
    "    Z = Z.reshape(100, 100)\n",
    "    CS = ax.contour(X, Y, Z)\n",
    "    ax.clabel(CS, inline=1, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_function(f, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to minimize $f$ using gradient descent, with optional flavors. For this, we define a minimize function that performs gradient descent, along with a helper class `GradientDescent` that will perform the updates given the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, params, lr=0.1):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.params:\n",
    "                param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(f, optimizer, max_iter=500, verbose=False):\n",
    "    if hasattr(optimizer, 'params'):\n",
    "        [iterate] = optimizer.params\n",
    "    else:\n",
    "        # For pytorch optimizers.\n",
    "        assert len(optimizer.param_groups) == 1\n",
    "        [iterate] = optimizer.param_groups[0]['params']\n",
    "    iterate_record = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # iterate.grad may be non zero. We zero it first:\n",
    "        optimizer.zero_grad()\n",
    "        value = f(iterate)\n",
    "        \n",
    "        # Compute the gradient of f with respect to the parameters:\n",
    "        value.backward()\n",
    "        # iterate.grad now holds $\\nabla_x f(x)$\n",
    "        if float(torch.sum(iterate.grad ** 2)) < 1e-6:\n",
    "            if verbose:\n",
    "                print('Converged at iteration %i: '\n",
    "                      'f(x) = %e, x = [%e, %e]'\n",
    "                      % (i, value, iterate[0], iterate[1]))\n",
    "            break\n",
    "        \n",
    "        # We store the trajectory of the iterates\n",
    "        iterate_record.append(iterate.data.clone()[None, :])\n",
    "        if verbose and i % 10 == 0:\n",
    "            print('Iteration %i: f(x) = %e, x = [%e, %e]'\n",
    "                  % (i, value, iterate[0], iterate[1]))\n",
    "\n",
    "        # Perform the parameter update step using the gradient\n",
    "        # values:\n",
    "        optimizer.step()\n",
    "    return torch.cat(iterate_record, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the minimization algorithm and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extra dimension marked with `None` is used to make it \n",
    "# possible\n",
    "init = torch.tensor([0.8, 0.8], dtype=torch.float32)\n",
    "optimizer = GradientDescent([init.clone().requires_grad_()], lr=0.1)\n",
    "iterate_rec = minimize(f, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace(iterate_rec, ax, label='', tags=True):\n",
    "    iterate_rec = iterate_rec.numpy()\n",
    "    n_steps = len(iterate_rec)\n",
    "    line = ax.plot(iterate_rec[:, 0], iterate_rec[:, 1], linestyle=':',\n",
    "                   marker='o', markersize=2,\n",
    "                   label=label + \" (%d steps)\" % n_steps)\n",
    "    color = plt.getp(line[0], 'color')\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", ec=color, fc='white',\n",
    "                      lw=1)\n",
    "    if tags:\n",
    "        # Put tags every 10 iterates. The small randomization makes it\n",
    "        # easier to spot tag overlap when the steps are very small.\n",
    "        for i in range(0, len(iterate_rec), 10):\n",
    "            ax.annotate(i, xy=(iterate_rec[i, 0], iterate_rec[i, 1]),\n",
    "                        xycoords='data',\n",
    "                        xytext=(5 + np.random.uniform(-2, 2),\n",
    "                                5 + np.random.uniform(-2, 2)),\n",
    "                        textcoords='offset points',\n",
    "                        bbox=bbox_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "plot_function(f, ax1)\n",
    "plot_function(f, ax2)\n",
    "plot_trace(iterate_rec, ax1, label='gradient_descent')\n",
    "plot_trace(iterate_rec, ax2, label='gradient_descent', tags=False)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([f(x) for x in iterate_rec.numpy()])\n",
    "plt.xlabel('optimization step')\n",
    "plt.ylabel('objective function value');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "- Try to move the initialization point to the other side of the yellow mountain, for instance at position `[0.8, 1.2]`. What do you observe? How to do you explain this?\n",
    "\n",
    "- Put back the init to `[0.8, 0.8]` and implement the step method of `MomemtumGradientDescent` in the following cell,\n",
    "- Check that it behaves as `GradientDescent` for `momemtum=0`\n",
    "- Can you find a value of `momentum` that makes it converge faster than gradient descent on for this objective function? Try with different values of `lr` and `momentum`.\n",
    "- Try to use [torch.optim.Adam](http://pytorch.org/docs/master/optim.html#torch.optim.Adam) in the minimization loop.\n",
    "- Compare the three trajectories.\n",
    "\n",
    "\n",
    "For the momentum let's use the following 2-stage mathematical description of the update step:\n",
    "\n",
    "$$ \\mathbf{v} \\leftarrow \\mu \\cdot \\mathbf{v} + \\nabla_\\mathbf{x}\\ell(\\mathbf{x}) $$\n",
    "$$ \\mathbf{x} \\leftarrow \\eta \\cdot \\mathbf{v} $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\eta$ is the learning rate coefficient (`lr` in Python code);\n",
    "- $\\mu$ is the momentum coefficient (set to 0.9 by default);\n",
    "- $\\nabla_\\mathbf{x}\\ell(\\mathbf{x})$ is the gradient of the loss function for the current value of the parameters $\\mathbf{x}$;\n",
    "- $\\mathbf{v}$ is the tensor of velocities and as the same shape as the parameters tensor $\\theta$. $v$ is initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumGradientDescent(GradientDescent):\n",
    "\n",
    "    def __init__(self, params, lr=0.1, momentum=.9):\n",
    "        super(MomentumGradientDescent, self).__init__(params, lr)\n",
    "        self.momentum = momentum\n",
    "        self.velocities = [torch.zeros_like(param, requires_grad=False)\n",
    "                           for param in params]\n",
    "\n",
    "    def step(self):\n",
    "        # TODO: implement me!\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/momentum_optimizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "init = torch.FloatTensor([0.8, 0.8])\n",
    "optimizer = GradientDescent([init.clone().requires_grad_()], lr=lr)\n",
    "iterate_rec_gd = minimize(f, optimizer)\n",
    "\n",
    "optimizer = MomentumGradientDescent([init.clone().requires_grad_()],\n",
    "                                    lr=lr, momentum=0.9)\n",
    "iterate_rec_mom = minimize(f, optimizer)\n",
    "\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 8))\n",
    "plot_function(f, ax0)\n",
    "plot_trace(iterate_rec_gd, ax0, label='gradient descent', tags=False)\n",
    "plot_trace(iterate_rec_mom, ax0, label='momentum', tags=False)\n",
    "ax0.legend();\n",
    "\n",
    "ax1.plot([f(x) for x in iterate_rec_gd.numpy()], label='gradient descent')\n",
    "ax1.plot([f(x) for x in iterate_rec_mom.numpy()], label='momentum')\n",
    "ax1.set_xlabel('optimization step')\n",
    "ax1.set_ylabel('objective function value')\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe ? Try changing the momentum and the initialization to compare optimization traces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
