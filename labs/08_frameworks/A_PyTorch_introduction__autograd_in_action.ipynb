{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Pytorch\n",
    "\n",
    "PyTorch is a popular deep learning framework that was introduced around one year ago. It has a slightly lower-level API than Keras, but is much easier to use than TensorFlow when it comes to defining custom model. It is very popular among researchers and kagglers - a little less in the industry (yet).\n",
    "\n",
    "http://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principles\n",
    "\n",
    "PyTorch essentially provides the user with two main utilies:\n",
    "\n",
    "- It allows to perform Tensor/vector computation on the GPU with an API similar (but not compatible) to Numpy.\n",
    "\n",
    "- It records all computation to be able to backpropagate through them. That is, provided a sequence of operations that starts from a tensor $\\theta$ to define a scalar $g(\\theta)$, it is able to compute \n",
    "$\\nabla_\\theta g(\\theta)$ exactly, with only one function call.\n",
    "\n",
    "Typically, $\\theta$ will be a parameter of a neural network and $g$ a loss function such as $\\ell(f_{\\theta}(x), y)$ for supervised learning. \n",
    "\n",
    "The essential difference with TensorFlow lies in the way $g$ is defined: in PyTorch, every node in the computation graph is done while executing the forward pass, from within the Python interpreter: any Numpy code can be ported to PyTorch easily, and all flow control operation (e.g. loops, if/else, etc.) can be kept untouched. In contrast, TensorFlow requires the user to define a graph more declaratively. This graph is then used internally (i.e. outside the Python interpreter), to compute both the the predictions, the loss value and its derivatives.\n",
    "\n",
    "PyTorch takes care of recording everything it needs to do the backpropagation, *on the fly*.\n",
    "\n",
    "Note that recent versions of TensorFlow (1.5 and later) now come with the [eager mode](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/g3doc/guide.md) to make TensorFlow use the define-by-run semantics of PyTorch (and Chainer). However this mode is quite new and still experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "We will use the CPU only version for the moment. We will assume that you have a working Anaconda environment.\n",
    "\n",
    "### On Linux:\n",
    "\n",
    "`conda install pytorch-cpu torchvision -c pytorch`\n",
    "\n",
    "If you prefer to use pip, see: http://pytorch.org.\n",
    "\n",
    "### On OSX\n",
    "\n",
    "`conda install pytorch torchvision -c pytorch`\n",
    "\n",
    "If you prefer to use pip, see: http://pytorch.org.\n",
    "\n",
    "### On Windows\n",
    "\n",
    "`conda install -c peterjc123 pytorch-cpu`\n",
    "\n",
    "Refer to https://github.com/peterjc123/pytorch-scripts for Windows installation problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the device\n",
    "PyTorch can run on either CPU or GPU. The new API (v0.4.0) lets us define it in a nice way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Two options: 'cuda' (for GPU) or 'cpu' (if no GPU available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Device type set:\", \"GPU\" if device.type == \"cuda\" else \"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first example\n",
    "\n",
    "We will define a vector $x \\in \\mathbb{R}^n$ and computes its norms using PyTorch. For this, we will define our first `Tensor` -- the Tensor object is the central element of PyTorch.\n",
    "\n",
    "Let us fill this vector with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "n = 3\n",
    "x = torch.rand(n, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x` is a tensor, which can be manipulated roughly as a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a small API tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go back and forth from numpy to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = torch.from_numpy(np.ones((n, n)))\n",
    "\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all tensors are typed and that you can only do operations with tensors of the same type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(A, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.matmul(A.float(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.float() @ x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a norm function that returns the norm of $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return torch.sqrt(torch.sum(x ** 2, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now interested in computing $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$. \n",
    "\n",
    "\n",
    "**Exercise**:\n",
    "\n",
    "- Show that if $f(x) = || x ||_2$ then $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$.\n",
    "\n",
    "\n",
    "Assume we are too lazy to derive the analytical form of the gradient manually. Instead we will use the `autograd` facilities of PyTorch; the new API (v0.4.0) merged the Tensor and Variable classes. Before that, we needed to define a Variable class to get access to the `autograd` facilities. It is now made easier with the necessary methods included in the Tensor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did we require autograd for our tensor x?\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_()\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute the norm of f. All PyTorch functions can handle both Variables and Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the gradient of this scalar variable with respect to all the variables that were used to compute its value.\n",
    "\n",
    "The following `.backward()` call will assign `.grad` attributes to all Variables requires in $f$ computation for which a gradient is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient $\\nabla_x f(x)$ can be found in `x.grad`, which is also a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare it to the result of the evaluation of the analytical expression of the derivative of f(x) given x:  $\\nabla_x f(x) = \\frac{x}{|| x ||_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_grad = x / f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works ! You now know everything you need to know to use PyTorch. Note that, similar to Keras, PyTorch comes with a number of predefined functions that are useful in network definition. Check out http://pytorch.org/docs/ and tutorials for an overview of the tools you can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**\n",
    "\n",
    "- Write a function the pytorch code for function `g` that computes the cosine similarity of two variable vectors with float entries $\\mathbf{x}$ and $\\mathbf{y}$:\n",
    "\n",
    "$$g(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x}^T \\mathbf{y}}{|| \\mathbf{x} ||_2 || \\mathbf{y} ||_2 }$$\n",
    "\n",
    "- Use `torch.autograd` to compute the derivatives with respect to $\\mathbf{x} \\in \\mathbb{R}^3$ and $\\mathbf{y} \\in \\mathbb{R}^3$ for some values of your choice;\n",
    "\n",
    "- Compute $\\nabla_x g(x, y)$ and $\\nabla_y g(x, y)$ for some choice of $\\mathbf{x} = \\alpha \\cdot \\mathbf{y}$ with any $\\alpha \\in \\mathbb{R}$. Check that you can get the expected result with Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, y):\n",
    "    # TODO: fix the following code to implement the cosine\n",
    "    # similarity function instead.\n",
    "    return torch.sum(x) + torch.sum(y)\n",
    "\n",
    "\n",
    "x = torch.tensor([0, 1, 2], dtype=torch.float, requires_grad=True)\n",
    "y = torch.tensor([3, 0.9, 2.2], dtype=torch.float, requires_grad=True)\n",
    "\n",
    "cosine = g(x, y)\n",
    "cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cosine_autograd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/cosine_autograd_colinear.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reinitialize our two variables to non colinear values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 1, 2], dtype=torch.float, requires_grad=True)\n",
    "y = torch.tensor([3, 0.9, 2.2], dtype=torch.float, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following cells several times (use `ctrl-enter`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine = g(x, y)\n",
    "print(cosine)\n",
    "cosine.backward()\n",
    "x.data.add_(0.5 * x.grad.data)\n",
    "y.data.add_(0.5 * y.grad.data)\n",
    "x.grad.data.zero_()\n",
    "y.grad.data.zero_()\n",
    "print(\"x\", x)\n",
    "print(\"y\", y)\n",
    "print(x / y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing gradient descent methods\n",
    "\n",
    "In this notebook, we will use PyTorch to compare the different gradient methods and a toy 2D examples: we will try to find the minimum of the difference of two Gaussians. PyTorch provides a convenient wrapper named `nn.Module` to define parametrized functions, that we will use along this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "class Norm(nn.Module):\n",
    "\n",
    "    def __init__(self, p=2.):\n",
    "        super(Norm, self).__init__()\n",
    "        self.p = Parameter(torch.tensor([p], dtype=torch.float))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        p_sum = torch.sum(torch.pow(x, self.p), dim=0)\n",
    "        return torch.pow(p_sum, 1 / self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.rand(n)\n",
    "norm = Norm(p=3.)\n",
    "v = norm(x)\n",
    "v.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access $\\nabla_p(x \\to || x ||_p)$ in `norm.p.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a Gaussian operator, along with a generic Gaussian combination. We will not consider the gradient w.r.t the parameters of these modules, hence we specify `requires_grad=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian(nn.Module):\n",
    "\n",
    "    def __init__(self, precision, mean):\n",
    "        super(Gaussian, self).__init__()\n",
    "\n",
    "        assert precision.shape == (2, 2)\n",
    "        assert mean.shape == (2,)\n",
    "\n",
    "        self.precision = Parameter(precision, requires_grad=False)\n",
    "        self.mean = Parameter(mean, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute the (unormalized) likelihood of x given a Gaussian.\n",
    "        \n",
    "        x can be a 2D-vector or a batch of 2D-vectors.\n",
    "        \"\"\"\n",
    "        xc = x - self.mean\n",
    "        if len(xc.shape) == 1:\n",
    "            # Reshape xc to work as a mini-batch.\n",
    "            xc = xc.view(1, -1)\n",
    "        value = torch.exp(-.5 * (torch.sum((xc @ self.precision) * xc, dim=1)))\n",
    "        return value\n",
    "\n",
    "\n",
    "class GaussianCombination(nn.Module):\n",
    "\n",
    "    def __init__(self, precisions, means, weights):\n",
    "        super(GaussianCombination, self).__init__()\n",
    "        assert len(precisions) == len(means) == len(weights)\n",
    "        self.gaussians = nn.ModuleList()\n",
    "        for precision, mean in zip(precisions, means):\n",
    "            self.gaussians.append(Gaussian(precision, mean))\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, x):\n",
    "        return sum(w * g(x) for g, w in zip(self.gaussians, self.weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define $f(x) = \\exp(-(x- m_1)^T P_1 (x - m_1)) - \\exp(-(x- m_2)^T P_2 (x - m_2))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = torch.tensor([[1, 0], [0, 4]], dtype=torch.float)\n",
    "m1 = torch.tensor([0, 1], dtype=torch.float)\n",
    "w1 = 1\n",
    "p2 = torch.tensor([[1, -2], [-2, 10]], dtype=torch.float)\n",
    "m2 = torch.tensor([0, -2], dtype=torch.float)\n",
    "w2 = - 1\n",
    "\n",
    "f = GaussianCombination([p1, p2], [m1, m2], [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a plotting function to visualize $f$. Note the small boilerplate to interface PyTorch with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_function(f, ax):\n",
    "    x_max, y_max, x_min, y_min = 3, 3, -3, -3\n",
    "    x = np.linspace(x_min, x_max, 100, dtype=np.float32)\n",
    "    y = np.linspace(y_min, y_max, 100, dtype=np.float32)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    samples = np.concatenate((X[:, :, None], Y[:, :, None]), axis=2)\n",
    "    samples = samples.reshape(-1, 2)\n",
    "    samples = torch.from_numpy(samples).requires_grad_()\n",
    "    Z = f(samples).data.numpy()\n",
    "    Z = Z.reshape(100, 100)\n",
    "    CS = ax.contour(X, Y, Z)\n",
    "    ax.clabel(CS, inline=1, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_function(f, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now try to minimize $f$ using gradient descent, with optional flavors. For this, we define a minimize function that performs gradient descent, along with a helper class `GradientDescent` that will perform the updates given the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, params, lr=0.1):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param.data = param.data - self.lr * param.grad.data\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(f, optimizer, max_iter=500, verbose=False):\n",
    "    if hasattr(optimizer, 'params'):\n",
    "        [iterate] = optimizer.params\n",
    "    else:\n",
    "        # For pytorch optimizers.\n",
    "        [iterate] = optimizer.param_groups[0]['params']\n",
    "    iterate_record = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # iterate.grad may be non zero. We zero it first:\n",
    "        optimizer.zero_grad()\n",
    "        value = f(iterate)\n",
    "        \n",
    "        # Compute the gradient of f with respect to the parameters:\n",
    "        value.backward()\n",
    "        # iterate.grad now holds $\\nabla_x f(x)$\n",
    "        if float(torch.sum(iterate.grad ** 2)) < 1e-6:\n",
    "            if verbose:\n",
    "                print(\"Converged!\")\n",
    "            break\n",
    "        \n",
    "        # We store the trajectory of the iterates\n",
    "        iterate_record.append(iterate.data.clone()[None, :])\n",
    "        if verbose:\n",
    "            print('Iteration %i: f(x) = %e, x = [%e, %e]'\n",
    "                  % (i, value, iterate[0], iterate[1]))\n",
    "\n",
    "        # Perform the parameter update step using the gradient\n",
    "        # values:\n",
    "        optimizer.step()\n",
    "    iterate_record = torch.cat(iterate_record, dim=0)\n",
    "    return iterate_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the minimization algorithm and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The extra dimension marked with `None` is used to make it \n",
    "# possible\n",
    "init = torch.tensor([0.8, 0.8], dtype=torch.float)\n",
    "optimizer = GradientDescent([init.clone().requires_grad_()], lr=0.1)\n",
    "iterate_rec = minimize(f, optimizer, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trace(iterate_rec, ax, label='', tags=True):\n",
    "    iterate_rec = iterate_rec.numpy()\n",
    "    n_steps = len(iterate_rec)\n",
    "    line = ax.plot(iterate_rec[:, 0], iterate_rec[:, 1], linestyle=':',\n",
    "                   marker='o', markersize=2,\n",
    "                   label=label + \" (%d steps)\" % n_steps)\n",
    "    color = plt.getp(line[0], 'color')\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", ec=color, fc='white',\n",
    "                      lw=1)\n",
    "    if tags:\n",
    "        for i in range(0, len(iterate_rec), 10):\n",
    "            ax.annotate(i, xy=(iterate_rec[i, 0], iterate_rec[i, 1]),\n",
    "                        xycoords='data',\n",
    "                        xytext=(5 + np.random.uniform(-2, 2),\n",
    "                                5 + np.random.uniform(-2, 2)),\n",
    "                        textcoords='offset points',\n",
    "                        bbox=bbox_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "plot_function(f, ax1)\n",
    "plot_function(f, ax2)\n",
    "plot_trace(iterate_rec, ax1, label='gradient_descent')\n",
    "plot_trace(iterate_rec, ax2, label='gradient_descent', tags=False)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "- Try to move the initialization point to the other side of the yellow mountain, for instance at position `[0.8, 1.2]`. What do you observe? How to do you explain this?\n",
    "- Implement the step method of `MomemtumGradientDescent`.\n",
    "- Check that it behaves as `GradientDescent` for `momemtum=0`\n",
    "- Can you find a value of `momentum` that makes it converge faster than gradient descent on for this objective function?\n",
    "- Try to use [torch.optim.Adam](http://pytorch.org/docs/master/optim.html#torch.optim.Adam) in the minimization loop.\n",
    "- Compare the three trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumGradientDescent(GradientDescent):\n",
    "\n",
    "    def __init__(self, params, lr=0.1, momentum=.9):\n",
    "        super(MomentumGradientDescent, self).__init__(params, lr)\n",
    "        self.momentum = momentum\n",
    "        self.velocities = [param.data.new(param.shape).zero_()\n",
    "                           for param in params]\n",
    "\n",
    "    def step(self):\n",
    "        # TODO: implement me!\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/momentum_optimizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_function(f, ax)\n",
    "\n",
    "lr = 0.1\n",
    "init = torch.FloatTensor([0.8, 0.8])\n",
    "\n",
    "optimizer = GradientDescent([init.clone().requires_grad_()], lr=lr)\n",
    "iterate_rec = minimize(f, optimizer)\n",
    "plot_trace(iterate_rec, ax, label='gradient descent', tags=False)\n",
    "\n",
    "optimizer = MomentumGradientDescent([init.clone().requires_grad_()],\n",
    "                                    lr=lr, momentum=0.9)\n",
    "iterate_rec = minimize(f, optimizer)\n",
    "plot_trace(iterate_rec, ax, label='momentum', tags=False)\n",
    "\n",
    "# TODO: plot torch.optim.Adam\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe ? Try changing the momentum and the initialization to compare optimization traces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
