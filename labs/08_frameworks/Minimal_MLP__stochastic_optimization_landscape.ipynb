{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic optimization landscape of a minimal MLP\n",
    "\n",
    "\n",
    "In this notebook, we will try to better understand how stochastic gradient works. We fit a very simple non-convex model to data generated from a linear ground truth model.\n",
    "\n",
    "We will also observe how the (stochastic) loss landscape changes when selecting different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.functional import mse_loss\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is generated from a simple model:\n",
    "$$y=  2x + \\epsilon$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\epsilon \\sim \\mathcal{N}(0, 3)$\n",
    "- $x \\sim \\mathcal{U}(-1, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_ground_truth(n_samples=100, std=0.1):\n",
    "    x = torch.FloatTensor(n_samples, 1).uniform_(-1, 1)\n",
    "    epsilon = torch.FloatTensor(n_samples, 1).normal_(0, std)\n",
    "    y = 2 * x + epsilon\n",
    "    return x, y\n",
    "\n",
    "n_samples = 100\n",
    "std = 3\n",
    "x, y = sample_from_ground_truth(n_samples=100, std=std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose a minimal single hidden layer perceptron model with a single hidden unit and no bias. The model has two tunable parameters $w_1$, and $w_2$, such that:\n",
    "\n",
    "$$f(x) = w_1 \\cdot \\sigma(w_2 \\cdot x)$$\n",
    "\n",
    "where $\\sigma$ is the ReLU function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, w=None):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.w1 = Parameter(torch.FloatTensor((1,)))\n",
    "        self.w2 = Parameter(torch.FloatTensor((1,)))\n",
    "        if w is None:\n",
    "            self.reset_parameters()\n",
    "        else:\n",
    "            self.set_parameters(w)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.w1.uniform_(-.1, .1)\n",
    "        self.w2.uniform_(-.1, .1)\n",
    "\n",
    "    def set_parameters(self, w):\n",
    "        with torch.no_grad():\n",
    "            self.w1[0] = w[0]\n",
    "            self.w2[0] = w[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w1 * relu(self.w2 * x)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebook, we define a function to sample from and plot loss landscapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import fabs\n",
    "\n",
    "\n",
    "def make_grids(x, y, model_constructor, expected_risk_func, grid_size=100):\n",
    "    n_samples = len(x)\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    # Grid logic\n",
    "    x_max, y_max, x_min, y_min = 5, 5, -5, -5\n",
    "    w1 = np.linspace(x_min, x_max, grid_size, dtype=np.float32)\n",
    "    w2 = np.linspace(y_min, y_max, grid_size, dtype=np.float32)\n",
    "    W1, W2 = np.meshgrid(w1, w2)\n",
    "    W = np.concatenate((W1[:, :, None], W2[:, :, None]), axis=2)\n",
    "    W = torch.from_numpy(W)\n",
    "\n",
    "    # We will store the results in this tensor\n",
    "    risks = torch.FloatTensor(n_samples, grid_size, grid_size)\n",
    "    expected_risk = torch.FloatTensor(grid_size, grid_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                model = model_constructor(W[i, j])\n",
    "                pred = model(x)\n",
    "                loss = mse_loss(pred, y, reduction=\"none\")\n",
    "                risks[:, i, j] = loss.view(-1)\n",
    "                expected_risk[i, j] = expected_risk_func(W[i, j, 0], W[i, j, 1])\n",
    "        empirical_risk = torch.mean(risks, dim=0)\n",
    "    \n",
    "    return W1, W2, risks.numpy(), empirical_risk.numpy(), expected_risk.numpy()\n",
    "\n",
    "\n",
    "def expected_risk_simple_mlp(w1, w2):\n",
    "    \"\"\"Question: Can you derive this your-self?\"\"\"\n",
    "    return .5 * (8 / 3 - (4 / 3) * w1 * w2 + 1 / 3 * w1 ** 2 * w2 ** 2) + std ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `risks[k, i, j]` holds loss value $\\ell(f(w_1^{(i)} , w_2^{(j)}, x_k), y_k)$ for a single data point $(x_k, y_k)$;\n",
    "\n",
    "- `empirical_risk[i, j]` corresponds to the empirical risk averaged over the training data points:\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{k=1}^{n} \\ell(f(w_1^{(i)}, w_2^{(j)}, x_k), y_k)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, W2, risks, empirical_risk, expected_risk = make_grids(\n",
    "    x, y, SimpleMLP, expected_risk_func=expected_risk_simple_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our train loop and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "def train(model, x, y, lr=.1, n_epochs=1):\n",
    "    optimizer = SGD(model.parameters(), lr=lr)\n",
    "    iterate_rec = []\n",
    "    grad_rec = []\n",
    "    for epoch in range(n_epochs):\n",
    "        # Iterate over the dataset one sample at a time:\n",
    "        # batch_size=1\n",
    "        for this_x, this_y in zip(x, y):\n",
    "            this_x = this_x[None, :]\n",
    "            this_y = this_y[None, :]\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(this_x)\n",
    "            loss = mse_loss(pred, this_y)\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                iterate_rec.append(\n",
    "                    [model.w1.clone()[0], model.w2.clone()[0]]\n",
    "                )\n",
    "                grad_rec.append(\n",
    "                    [model.w1.grad.clone()[0], model.w2.grad.clone()[0]]\n",
    "                )\n",
    "            optimizer.step()\n",
    "    return np.array(iterate_rec), np.array(grad_rec)\n",
    "\n",
    "init = torch.FloatTensor([3, -4])\n",
    "model = SimpleMLP(init)\n",
    "iterate_rec, grad_rec = train(model, x, y, lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterate_rec[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot:\n",
    "- the point-wise risk at iteration $k$ on the left plot\n",
    "- the total empirical risk on the center plot\n",
    "- the expected risk on the right plot\n",
    "\n",
    "Observe how empirical and expected risk differ, and how empirical risk minimization is not totally equivalent to expected risk minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as colors\n",
    "\n",
    "\n",
    "class LevelsNormalize(colors.Normalize):\n",
    "    def __init__(self, levels, clip=False):\n",
    "        self.levels = levels\n",
    "        vmin, vmax = levels[0], levels[-1]\n",
    "        colors.Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        quantiles = np.linspace(0, 1, len(self.levels))\n",
    "        return np.ma.masked_array(np.interp(value, self.levels, quantiles))\n",
    "\n",
    "\n",
    "def plot_map(W1, W2, risks, emp_risk, exp_risk, sample, iter_):\n",
    "    all_risks = np.concatenate((emp_risk.ravel(), exp_risk.ravel()))\n",
    "    x_center, y_center = emp_risk.shape[0] // 2, emp_risk.shape[1] // 2\n",
    "    risk_at_center = exp_risk[x_center, y_center]\n",
    "    low_levels = np.percentile(all_risks[all_risks <= risk_at_center],\n",
    "                               q=np.linspace(0, 100, 11))\n",
    "    high_levels = np.percentile(all_risks[all_risks > risk_at_center],\n",
    "                                q=np.linspace(10, 100, 10))\n",
    "    levels = np.concatenate((low_levels, high_levels))\n",
    "    norm = LevelsNormalize(levels=levels)\n",
    "\n",
    "    cmap = plt.get_cmap('RdBu_r')\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(12, 4))\n",
    "    risk_levels = levels.copy()\n",
    "    risk_levels[0] = min(risks[sample].min(), risk_levels[0])\n",
    "    risk_levels[-1] = max(risks[sample].max(), risk_levels[-1])\n",
    "    ax1.contourf(W1, W2, risks[sample], levels=risk_levels,\n",
    "                 norm=norm, cmap=cmap)\n",
    "    ax1.scatter(iterate_rec[iter_, 0], iterate_rec[iter_, 1],\n",
    "                color='orange')\n",
    "    if any(grad_rec[iter_] != 0):\n",
    "        ax1.arrow(iterate_rec[iter_, 0], iterate_rec[iter_, 1],\n",
    "                  -0.1 * grad_rec[iter_, 0], -0.1 * grad_rec[iter_, 1],\n",
    "                  head_width=0.3, head_length=0.5, fc='orange', ec='orange')\n",
    "    ax1.set_title('Pointwise risk')\n",
    "    ax2.contourf(W1, W2, emp_risk, levels=levels, norm=norm, cmap=cmap)\n",
    "    ax2.plot(iterate_rec[:iter_ + 1, 0], iterate_rec[:iter_ + 1, 1],\n",
    "             linestyle='-', marker='o', markersize=6,\n",
    "             color='orange', linewidth=2, label='SGD trajectory')\n",
    "    ax2.legend()\n",
    "    ax2.set_title('Empirical risk')\n",
    "    cf = ax3.contourf(W1, W2, exp_risk, levels=levels, norm=norm, cmap=cmap)\n",
    "    ax3.scatter(iterate_rec[iter_, 0], iterate_rec[iter_, 1],\n",
    "                color='orange', label='Current sample')\n",
    "    ax3.set_title('Expected risk (ground truth)')\n",
    "    plt.colorbar(cf, ax=ax3)\n",
    "    ax3.legend()\n",
    "    fig.suptitle('Iter %i, sample % i' % (iter_, sample))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(0, 100, 10):\n",
    "    plot_map(W1, W2, risks, empirical_risk, expected_risk, sample, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe and comment.\n",
    "\n",
    "### Exercices:\n",
    "\n",
    "- Change the model to a completely linear one and reproduce the plots. What change do you observe regarding the plot of the stochastic loss landscape?\n",
    "- Try to initialize the model with pathological weights, e.g., symmetric ones. What do you observe?\n",
    "    - You may increase the number of epochs to observe slow convergence phenomena\n",
    "- Try augmenting the noise in the dataset. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/linear_mlp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities to generate the slides figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.animation import FuncAnimation\n",
    "# from IPython.display import HTML\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(8, 8))\n",
    "# all_risks = np.concatenate((empirical_risk.ravel(),\n",
    "#                             expected_risk.ravel()))\n",
    "\n",
    "# x_center, y_center = empirical_risk.shape[0] // 2, empirical_risk.shape[1] // 2\n",
    "# risk_at_center = expected_risk[x_center, y_center]\n",
    "# low_levels = np.percentile(all_risks[all_risks <= risk_at_center],\n",
    "#                            q=np.linspace(0, 100, 11))\n",
    "# high_levels = np.percentile(all_risks[all_risks > risk_at_center],\n",
    "#                             q=np.linspace(10, 100, 10))\n",
    "# levels = np.concatenate((low_levels, high_levels))\n",
    "# norm = LevelsNormalize(levels=levels)\n",
    "\n",
    "# cmap = plt.get_cmap('RdBu_r')\n",
    "\n",
    "# ax.set_title('Pointwise risk')\n",
    "\n",
    "\n",
    "# def animate(i):\n",
    "#     for c in ax.collections:\n",
    "#         c.remove()\n",
    "#     for l in ax.lines:\n",
    "#         l.remove()\n",
    "#     for p in ax.patches:\n",
    "#         p.remove()\n",
    "#     risk_levels = levels.copy()\n",
    "#     risk_levels[0] = min(risks[i].min(), risk_levels[0])\n",
    "#     risk_levels[-1] = max(risks[i].max(), risk_levels[-1])\n",
    "#     ax.contourf(W1, W2, risks[i], levels=risk_levels,\n",
    "#                  norm=norm, cmap=cmap)\n",
    "#     ax.plot(iterate_rec[:i + 1, 0], iterate_rec[:i + 1, 1],\n",
    "#              linestyle='-', marker='o', markersize=6,\n",
    "#              color='orange', linewidth=2, label='SGD trajectory')\n",
    "#     return []\n",
    "\n",
    "# anim = FuncAnimation(fig, animate,# init_func=init,\n",
    "#                      frames=100, interval=300, blit=True)\n",
    "# anim.save(\"stochastic_landscape_minimal_mlp.mp4\")\n",
    "# plt.close(fig)\n",
    "# HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(8, 7))\n",
    "# cf = ax.contourf(W1, W2, empirical_risk, levels=levels, norm=norm, cmap=cmap)\n",
    "# ax.plot(iterate_rec[:100 + 1, 0], iterate_rec[:100 + 1, 1],\n",
    "#          linestyle='-', marker='o', markersize=6,\n",
    "#          color='orange', linewidth=2, label='SGD trajectory')\n",
    "# ax.legend()\n",
    "# plt.colorbar(cf, ax=ax)\n",
    "# ax.set_title('Empirical risk')\n",
    "# fig.savefig('empirical_loss_landscape_minimal_mlp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
