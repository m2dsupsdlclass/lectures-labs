{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Feedback Neural Recommender Systems\n",
    "\n",
    "Goals:\n",
    "- Understand recommender data\n",
    "- Build different models architectures using Keras\n",
    "- Retrieve Embeddings and visualize them\n",
    "- Add metadata information as input to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "\n",
    "from zipfile import ZipFile\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2 compat\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "\n",
    "ML_100K_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "ML_100K_FILENAME = ML_100K_URL.rsplit('/', 1)[1]\n",
    "ML_100K_FOLDER = 'ml-100k'\n",
    "\n",
    "if not op.exists(ML_100K_FILENAME):\n",
    "    print('Downloading %s to %s...' % (ML_100K_URL, ML_100K_FILENAME))\n",
    "    urlretrieve(ML_100K_URL, ML_100K_FILENAME)\n",
    "\n",
    "if not op.exists(ML_100K_FOLDER):\n",
    "    print('Extracting %s to %s...' % (ML_100K_FILENAME, ML_100K_FOLDER))\n",
    "    ZipFile(ML_100K_FILENAME).extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings file\n",
    "\n",
    "Each line contains a rated movie: \n",
    "- a user\n",
    "- an item\n",
    "- a rating from 1 to 5 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_ratings = pd.read_csv(op.join(ML_100K_FOLDER, 'u.data'), sep='\\t',\n",
    "                      names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "raw_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item metadata file\n",
    "\n",
    "The item metadata file contains metadata like the name of the movie or the date it was released. The movies file contains columns indicating the movie's genres. Let's only load the first five columns of the file with `usecols`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cols = ['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "items = pd.read_csv(op.join(ML_100K_FOLDER, 'u.item'), sep='|',\n",
    "                    names=m_cols, usecols=range(5), encoding='latin-1')\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a bit of Python preprocessing code to extract the release year as an integer value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(release_date):\n",
    "    if hasattr(release_date, 'split'):\n",
    "        components = release_date.split('-')\n",
    "        if len(components) == 3:\n",
    "            return int(components[2])\n",
    "    # Missing value marker\n",
    "    return 1920\n",
    "\n",
    "\n",
    "items['release_year'] = items['release_date'].map(extract_year)\n",
    "items.hist('release_year', bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrich the raw ratings data with the collected items metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.merge(items, raw_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "To understand well the distribution of the data, the following statistics are computed:\n",
    "- the number of users\n",
    "- the number of items\n",
    "- the rating distribution\n",
    "- the popularity of each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_user_id = all_ratings['user_id'].max()\n",
    "max_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_item_id = all_ratings['item_id'].max()\n",
    "max_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings['rating'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a bit more pandas magic compute the popularity of each movie (number of ratings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity = all_ratings.groupby('item_id').size().reset_index(name='popularity')\n",
    "items = pd.merge(popularity, items)\n",
    "items.nlargest(10, 'popularity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrich the ratings data with the popularity as an additional metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ratings = pd.merge(popularity, all_ratings)\n",
    "all_ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later in the analysis we will assume that this popularity does not come from the ratings themselves but from an external metadata, e.g. box office numbers in the month after the release in movie theaters.\n",
    "\n",
    "Let's split the enriched data in a train / test split to make it possible to do predictive modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ratings_train, ratings_test = train_test_split(\n",
    "    all_ratings, test_size=0.2, random_state=0)\n",
    "\n",
    "user_id_train = np.array(ratings_train['user_id'])\n",
    "item_id_train = np.array(ratings_train['item_id'])\n",
    "rating_train = np.array(ratings_train['rating'])\n",
    "\n",
    "user_id_test = np.array(ratings_test['user_id'])\n",
    "item_id_test = np.array(ratings_test['item_id'])\n",
    "rating_test = np.array(ratings_test['rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit feedback: supervised ratings prediction\n",
    "\n",
    "For each pair of (user, item) try to predict the rating the user would give to the item.\n",
    "\n",
    "This is the classical setup for building recommender systems from offline data with explicit supervision signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive ratings  as a regression problem\n",
    "\n",
    "The following code implements the following architecture:\n",
    "\n",
    "<img src=\"images/rec_archi_1.svg\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample we input the integer identifiers\n",
    "# of a single user and a single item\n",
    "class RegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user_id, max_item_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                                        input_length=1, name='user_embedding')\n",
    "        self.item_embedding = Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                                        input_length=1, name='item_embedding')\n",
    "        \n",
    "        # The following two layers don't have parameters.\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        item_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        item_vecs = self.flatten(self.item_embedding(item_inputs))\n",
    "        \n",
    "        y = self.dot([user_vecs, item_vecs])\n",
    "        return y\n",
    "        \n",
    "model = RegressionModel(30, max_user_id, max_item_id)\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful for debugging the output shape of model\n",
    "initial_train_preds = model.predict([user_id_train, item_id_train])\n",
    "initial_train_preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model error\n",
    "\n",
    "Using `initial_train_preds`, compute the model errors:\n",
    "- mean absolute error\n",
    "- mean squared error\n",
    "\n",
    "Converting a pandas Series to numpy array is usually implicit, but you may use `rating_train.values` to do so explicitly. Be sure to monitor the shapes of each object you deal with by using `object.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/compute_errors.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring runs\n",
    "\n",
    "Keras enables to monitor various variables during training. \n",
    "\n",
    "`history.history` returned by the `model.fit` function is a dictionary\n",
    "containing the `'loss'` and validation loss `'val_loss'` after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Training the model\n",
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=64, epochs=6, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "\n",
    "- Why is the train loss higher than the first loss in the first few epochs?\n",
    "- Why is Keras not computing the train loss on the full training set at the end of each epoch as it does on the validation set?\n",
    "\n",
    "\n",
    "Now that the model is trained, the model MSE and MAE look nicer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict([user_id_train, item_id_train])\n",
    "print(\"Final train MSE: %0.3f\" % mean_squared_error(train_preds, rating_train))\n",
    "print(\"Final train MAE: %0.3f\" % mean_absolute_error(train_preds, rating_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Deep recommender model\n",
    "\n",
    "Using a similar framework as previously, the following deep model described in the course was built (with only two fully connected)\n",
    "\n",
    "<img src=\"images/rec_archi_2.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "To build this model we will need a new kind of layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise\n",
    "\n",
    "- The following code has **4 errors** that prevent it from working correctly. **Correct them and explain** why they are critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sample we input the integer identifiers\n",
    "# of a single user and a single item\n",
    "class DeepRegressionModel(Model):\n",
    "    def __init__(self, embedding_size, max_user_id, max_item_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                                        input_length=1, name='user_embedding')\n",
    "        self.item_embedding = Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                                        input_length=1, name='item_embedding')\n",
    "        \n",
    "        # The following two layers don't have parameters.\n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        \n",
    "        self.dropout = Dropout(0.99)\n",
    "        self.dense1 = Dense(64, activation=\"relu\")\n",
    "        self.dense2 = Dense(2, activation=\"tanh\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        item_inputs = inputs[1]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        item_vecs = self.flatten(self.item_embedding(item_inputs))\n",
    "        \n",
    "        input_vecs = self.concat([user_vecs, item_vecs])\n",
    "        \n",
    "        y = self.dropout(input_vecs)\n",
    "        y = self.dense1(y)\n",
    "        y = self.dense2(y)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "model = DeepRegressionModel(30, max_user_id, max_item_id)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "initial_train_preds = model.predict([user_id_train, item_id_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/deep_explicit_feedback_recsys.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit([user_id_train, item_id_train], rating_train,\n",
    "                    batch_size=64, epochs=5, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.ylim(0, 2)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Loss');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = model.predict([user_id_train, item_id_train])\n",
    "print(\"Final train MSE: %0.3f\" % mean_squared_error(train_preds, rating_train))\n",
    "print(\"Final train MAE: %0.3f\" % mean_absolute_error(train_preds, rating_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home assignment: \n",
    " - Add another layer, compare train/test error\n",
    " - What do you notice? \n",
    " - Try adding more dropout and modifying layer sizes: should you increase\n",
    "   or decrease the number of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Embeddings\n",
    "\n",
    "- It is possible to retrieve the embeddings by simply using the Keras function `model.get_weights` which returns all the model learnable parameters.\n",
    "- The weights are returned the same order as they were build in the model\n",
    "- What is the total number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights and shape\n",
    "weights = model.get_weights()\n",
    "[w.shape for w in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: \n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = weights[0]\n",
    "item_embeddings = weights[1]\n",
    "print(\"First item name from metadata:\", items[\"title\"][1])\n",
    "print(\"Embedding vector for the first item:\")\n",
    "print(item_embeddings[1])\n",
    "print(\"shape:\", item_embeddings[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding most similar items\n",
    "Finding k most similar items to a point in embedding space\n",
    "\n",
    "- Write in numpy a function to compute the cosine similarity between two points in embedding space\n",
    "- Write a function which computes the euclidean distance between a point in embedding space and all other points\n",
    "- Write a most similar function, which returns the k item names with lowest euclidean distance\n",
    "- Try with a movie index, such as 181 (Return of the Jedi). What do you observe? Don't expect miracles on such a small training set.\n",
    "\n",
    "Notes:\n",
    "- you may use `np.linalg.norm` to compute the norm of vector, and you may specify the `axis=`\n",
    "- the numpy function `np.argsort(...)` enables to compute the sorted indices of a vector\n",
    "- `items[\"name\"][idxs]` returns the names of the items indexed by array idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-07\n",
    "\n",
    "def cosine(x, y):\n",
    "    # TODO: modify function\n",
    "    return 0.\n",
    "\n",
    "# Computes euclidean distances between x and all item embeddings\n",
    "def euclidean_distances(x):\n",
    "    # TODO: modify function\n",
    "    return 0.\n",
    "\n",
    "# Computes top_n most similar items to an idx\n",
    "def most_similar(idx, top_n=10):\n",
    "    # TODO: modify function\n",
    "    idxs = np.array([1,2,3])\n",
    "    return items[\"title\"][idxs]\n",
    "\n",
    "most_similar(181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/similarity.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing embeddings using TSNE\n",
    "\n",
    "- we use scikit learn to visualize items embeddings\n",
    "- Try different perplexities, and visualize user embeddings as well\n",
    "- What can you conclude ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "item_tsne = TSNE(perplexity=30).fit_transform(item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(item_tsne[:, 0], item_tsne[:, 1]);\n",
    "plt.xticks(()); plt.yticks(());\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively with [Uniform Manifold Approximation and Projection](https://github.com/lmcinnes/umap):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "\n",
    "# item_umap = umap.UMAP().fit_transform(item_embeddings)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.scatter(item_umap[:, 0], item_umap[:, 1]);\n",
    "# plt.xticks(()); plt.yticks(());\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using item metadata in the model\n",
    "\n",
    "Using a similar framework as previously, we will build another deep model that can also leverage additional metadata. The resulting system is therefore an **Hybrid Recommender System** that does both **Collaborative Filtering** and **Content-based recommendations**.\n",
    "\n",
    "<img src=\"images/rec_archi_3.svg\" style=\"width: 600px;\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "meta_columns = ['popularity', 'release_year']\n",
    "\n",
    "scaler = QuantileTransformer()\n",
    "item_meta_train = scaler.fit_transform(ratings_train[meta_columns])\n",
    "item_meta_test = scaler.transform(ratings_test[meta_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridModel(Model):\n",
    "    def __init__(self, embedding_size, max_user_id, max_item_id):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.user_embedding = Embedding(output_dim=embedding_size, input_dim=max_user_id + 1,\n",
    "                                        input_length=1, name='user_embedding')\n",
    "        self.item_embedding = Embedding(output_dim=embedding_size, input_dim=max_item_id + 1,\n",
    "                                        input_length=1, name='item_embedding')\n",
    "        \n",
    "        # The following two layers don't have parameters.\n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "        \n",
    "        self.dense1 = Dense(64, activation=\"relu\")\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.dense2 = Dense(32, activation='relu')\n",
    "        self.dense3 = Dense(2, activation=\"tanh\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_inputs = inputs[0]\n",
    "        item_inputs = inputs[1]\n",
    "        meta_inputs = inputs[2]\n",
    "        \n",
    "        user_vecs = self.flatten(self.user_embedding(user_inputs))\n",
    "        item_vecs = self.flatten(self.item_embedding(item_inputs))\n",
    "        \n",
    "        input_vecs = self.concat([user_vecs, item_vecs, meta_inputs])\n",
    "        \n",
    "        y = self.dense1(input_vecs)\n",
    "        y = self.dropout(y)\n",
    "        y = self.dense2(y)\n",
    "        y = self.dense3(y)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "model = HybridModel(30, max_user_id, max_item_id)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "initial_train_preds = model.predict([user_id_train, item_id_train, item_meta_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit([user_id_train, item_id_train, item_meta_train], rating_train,\n",
    "                    batch_size=64, epochs=15, validation_split=0.1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict([user_id_test, item_id_test, item_meta_test])\n",
    "print(\"Final test MSE: %0.3f\" % mean_squared_error(test_preds, rating_test))\n",
    "print(\"Final test MAE: %0.3f\" % mean_absolute_error(test_preds, rating_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The additional metadata seem to improve the predictive power of the model a bit at least in terms of MAE.\n",
    "\n",
    "\n",
    "### A recommendation function for a given user\n",
    "\n",
    "Once the model is trained, the system can be used to recommend a few items for a user, that he/she hasn't already seen:\n",
    "- we use the `model.predict` to compute the ratings a user would have given to all items\n",
    "- we build a reco function that sorts these items and exclude those the user has already seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indexed_items = items.set_index('item_id')\n",
    "\n",
    "\n",
    "def recommend(user_id, top_n=10):\n",
    "    item_ids = range(1, max_item_id)\n",
    "    seen_mask = all_ratings[\"user_id\"] == user_id\n",
    "    seen_movies = set(all_ratings[seen_mask][\"item_id\"])\n",
    "    item_ids = list(filter(lambda x: x not in seen_movies, item_ids))\n",
    "\n",
    "    print(\"User %d has seen %d movies, including:\" % (user_id, len(seen_movies)))\n",
    "    for title in all_ratings[seen_mask].nlargest(20, 'popularity')['title']:\n",
    "        print(\"   \", title)\n",
    "    print(\"Computing ratings for %d other movies:\" % len(item_ids))\n",
    "    \n",
    "    item_ids = np.array(item_ids)\n",
    "    user_ids = np.zeros_like(item_ids)\n",
    "    user_ids[:] = user_id\n",
    "    items_meta = scaler.transform(indexed_items[meta_columns].loc[item_ids])\n",
    "    \n",
    "    rating_preds = model.predict([user_ids, item_ids, items_meta])\n",
    "    \n",
    "    item_ids = np.argsort(rating_preds[:, 0])[::-1].tolist()\n",
    "    rec_items = item_ids[:top_n]\n",
    "    return [(items[\"title\"][movie], rating_preds[movie][0])\n",
    "            for movie in rec_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for title, pred_rating in recommend(5):\n",
    "    print(\"    %0.1f: %s\" % (pred_rating, title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Home assignment: Predicting ratings as a classification problem\n",
    "\n",
    "In this dataset, the ratings all belong to a finite set of possible values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(rating_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we can help the model by forcing it to predict those values by treating the problem as a multiclassification problem. The only required changes are:\n",
    "\n",
    "- setting the final layer to output class membership probabities using a softmax activation with 5 outputs;\n",
    "- optimize the categorical cross-entropy classification loss instead of a regression loss such as MSE or MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/classification.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
