{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss for Implicit Feedback Neural Recommender Systems\n",
    "\n",
    "The goal of this notebook is first to demonstrate how it is possible to build a bi-linear recommender system only using positive feedback data.\n",
    "\n",
    "In a latter section we show that it is possible to train deeper architectures following the same design principles.\n",
    "\n",
    "This notebook is inspired by Maciej Kula's [Recommendations in Keras using triplet loss](\n",
    "https://github.com/maciejkula/triplet_recommendations_keras). Contrary to Maciej we won't use the BPR loss but instead will introduce the more common margin-based comparator.\n",
    "\n",
    "## Loading the movielens-100k dataset\n",
    "\n",
    "For the sake of computation time, we will only use the smallest variant of the movielens reviews dataset. Beware that the architectural choices and hyperparameters that work well on such a toy dataset will not necessarily be representative of the behavior when run on a more realistic dataset such as [Movielens 10M](https://grouplens.org/datasets/movielens/10m/) or the [Yahoo Songs dataset with 700M rating](https://webscope.sandbox.yahoo.com/catalog.php?datatype=r)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as op\n",
    "\n",
    "from zipfile import ZipFile\n",
    "try:\n",
    "    from urllib.request import urlretrieve\n",
    "except ImportError:  # Python 2 compat\n",
    "    from urllib import urlretrieve\n",
    "\n",
    "\n",
    "ML_100K_URL = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "ML_100K_FILENAME = ML_100K_URL.rsplit('/', 1)[1]\n",
    "ML_100K_FOLDER = 'ml-100k'\n",
    "\n",
    "if not op.exists(ML_100K_FILENAME):\n",
    "    print('Downloading %s to %s...' % (ML_100K_URL, ML_100K_FILENAME))\n",
    "    urlretrieve(ML_100K_URL, ML_100K_FILENAME)\n",
    "\n",
    "if not op.exists(ML_100K_FOLDER):\n",
    "    print('Extracting %s to %s...' % (ML_100K_FILENAME, ML_100K_FOLDER))\n",
    "    ZipFile(ML_100K_FILENAME).extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90570.000000</td>\n",
       "      <td>90570.000000</td>\n",
       "      <td>90570.000000</td>\n",
       "      <td>9.057000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>461.494038</td>\n",
       "      <td>428.104891</td>\n",
       "      <td>3.523827</td>\n",
       "      <td>8.835073e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>266.004364</td>\n",
       "      <td>333.088029</td>\n",
       "      <td>1.126073</td>\n",
       "      <td>5.341684e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.747247e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>256.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.794484e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>442.000000</td>\n",
       "      <td>324.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.828143e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>682.000000</td>\n",
       "      <td>636.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.882049e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>943.000000</td>\n",
       "      <td>1682.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.932866e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user_id       item_id        rating     timestamp\n",
       "count  90570.000000  90570.000000  90570.000000  9.057000e+04\n",
       "mean     461.494038    428.104891      3.523827  8.835073e+08\n",
       "std      266.004364    333.088029      1.126073  5.341684e+06\n",
       "min        1.000000      1.000000      1.000000  8.747247e+08\n",
       "25%      256.000000    174.000000      3.000000  8.794484e+08\n",
       "50%      442.000000    324.000000      4.000000  8.828143e+08\n",
       "75%      682.000000    636.000000      4.000000  8.882049e+08\n",
       "max      943.000000   1682.000000      5.000000  8.932866e+08"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = pd.read_csv(op.join(ML_100K_FOLDER, 'ua.base'), sep='\\t',\n",
    "                        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "data_test = pd.read_csv(op.join(ML_100K_FOLDER, 'ua.test'), sep='\\t',\n",
    "                        names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"])\n",
    "\n",
    "data_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_year(release_date):\n",
    "    if hasattr(release_date, 'split'):\n",
    "        components = release_date.split('-')\n",
    "        if len(components) == 3:\n",
    "            return int(components[2])\n",
    "    # Missing value marker\n",
    "    return 1920\n",
    "\n",
    "\n",
    "m_cols = ['item_id', 'title', 'release_date', 'video_release_date', 'imdb_url']\n",
    "items = pd.read_csv(op.join(ML_100K_FOLDER, 'u.item'), sep='|',\n",
    "                    names=m_cols, usecols=range(5), encoding='latin-1')\n",
    "items['release_year'] = items['release_date'].map(extract_year)\n",
    "\n",
    "data_train = pd.merge(data_train, items)\n",
    "data_test = pd.merge(data_test, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>release_date</th>\n",
       "      <th>video_release_date</th>\n",
       "      <th>imdb_url</th>\n",
       "      <th>release_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>888550871</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>883599478</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>877888877</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>882140487</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>01-Jan-1995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://us.imdb.com/M/title-exact?Toy%20Story%2...</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp             title release_date  \\\n",
       "0        1        1       5  874965758  Toy Story (1995)  01-Jan-1995   \n",
       "1        2        1       4  888550871  Toy Story (1995)  01-Jan-1995   \n",
       "2        6        1       4  883599478  Toy Story (1995)  01-Jan-1995   \n",
       "3       10        1       4  877888877  Toy Story (1995)  01-Jan-1995   \n",
       "4       13        1       3  882140487  Toy Story (1995)  01-Jan-1995   \n",
       "\n",
       "   video_release_date                                           imdb_url  \\\n",
       "0                 NaN  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
       "1                 NaN  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
       "2                 NaN  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
       "3                 NaN  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
       "4                 NaN  http://us.imdb.com/M/title-exact?Toy%20Story%2...   \n",
       "\n",
       "   release_year  \n",
       "0          1995  \n",
       "1          1995  \n",
       "2          1995  \n",
       "3          1995  \n",
       "4          1995  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_users=944, n_items=1683\n"
     ]
    }
   ],
   "source": [
    "max_user_id = max(data_train['user_id'].max(), data_test['user_id'].max())\n",
    "max_item_id = max(data_train['item_id'].max(), data_test['item_id'].max())\n",
    "\n",
    "n_users = max_user_id + 1\n",
    "n_items = max_item_id + 1\n",
    "\n",
    "print('n_users=%d, n_items=%d' % (n_users, n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit feedback data\n",
    "\n",
    "Consider ratings >= 4 as positive feed back and ignore the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data_train = data_train.query(\"rating >= 4\")\n",
    "pos_data_test = data_test.query(\"rating >= 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the median rating is around 3.5, this cut will remove approximately half of the ratings from the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49906"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data_train['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5469"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_data_test['rating'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Triplet Loss\n",
    "\n",
    "The following section demonstrates how to build a low-rank quadratic interaction model between users and items. The similarity score between a user and an item is defined by the unormalized dot products of their respective embeddings.\n",
    "\n",
    "The matching scores can be use to rank items to recommend to a specific user.\n",
    "\n",
    "Training of the model parameters is achieved by randomly sampling negative items not seen by a pre-selected anchor user. We want the model embedding matrices to be such that the similarity between the user vector and the negative vector is smaller than the similarity between the user vector and the positive item vector. Furthermore we use a margin to further move appart the negative from the anchor user.\n",
    "\n",
    "Here is the architecture of such a triplet architecture. The triplet name comes from the fact that the loss to optimize is defined for triple `(anchor_user, positive_item, negative_item)`:\n",
    "\n",
    "<img src=\"images/rec_archi_implicit_2.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "We call this model a triplet model with bi-linear interactions because the similarity between a user and an item is captured by a dot product of the first level embedding vectors. This is therefore not a deep architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "\n",
    "class MarginLoss(layers.Layer):\n",
    "\n",
    "    def __init__(self, margin=1.):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        pos_pair_similarity = inputs[0]\n",
    "        neg_pair_similarity = inputs[1]\n",
    "        \n",
    "        diff = neg_pair_similarity - pos_pair_similarity\n",
    "        return tf.maximum(diff + self.margin, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the actual code that builds the model(s) with shared weights. Note that here we use the cosine similarity instead of unormalized dot products (both seems to yield comparable results).\n",
    "\n",
    "The triplet model is used to train the weights of the companion\n",
    "similarity model. The triplet model takes 1 user, 1 positive item\n",
    "(relative to the selected user) and one negative item and is\n",
    "trained with comparator loss.\n",
    "\n",
    "The similarity model takes one user and one item as input and return\n",
    "compatibility score (aka the match score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dense\n",
    "from tensorflow.keras.layers import Lambda, Dot\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class TripletModel(Model):\n",
    "    def __init__(self, n_users, n_items, latent_dim=64,\n",
    "                 l2_reg=None, margin=1.):\n",
    "        super().__init__(name=\"TripletModel\")\n",
    "        \n",
    "        self.margin = margin\n",
    "        \n",
    "        l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "\n",
    "        self.user_layer = Embedding(n_users, latent_dim,\n",
    "                                    input_length=1,\n",
    "                                    input_shape=(1,),\n",
    "                                    name='user_embedding',\n",
    "                                    embeddings_regularizer=l2_reg)\n",
    "    \n",
    "        # The following embedding parameters will be shared to\n",
    "        # encode both the positive and negative items.\n",
    "        self.item_layer = Embedding(n_items, latent_dim,\n",
    "                                    input_length=1,\n",
    "                                    name=\"item_embedding\",\n",
    "                                    embeddings_regularizer=l2_reg)\n",
    "        \n",
    "        # The 2 following layers are without parameters, and can\n",
    "        # therefore be used for both positive and negative items.\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1, normalize=True)\n",
    "\n",
    "        self.margin_loss = MarginLoss(margin)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        user_input = inputs[0]\n",
    "        pos_item_input = inputs[1]\n",
    "        neg_item_input = inputs[2]\n",
    "        \n",
    "        user_embedding = self.user_layer(user_input)\n",
    "        user_embedding = self.flatten(user_embedding)\n",
    "        \n",
    "        pos_item_embedding = self.item_layer(pos_item_input)\n",
    "        pos_item_embedding = self.flatten(pos_item_embedding)\n",
    "        \n",
    "        neg_item_embedding = self.item_layer(neg_item_input)\n",
    "        neg_item_embedding = self.flatten(neg_item_embedding)\n",
    "        \n",
    "        # Similarity computation between embeddings\n",
    "        pos_similarity = self.dot([user_embedding, pos_item_embedding])\n",
    "        neg_similarity = self.dot([user_embedding, neg_item_embedding])\n",
    "                \n",
    "        return self.margin_loss([pos_similarity, neg_similarity])\n",
    "    \n",
    "\n",
    "triplet_model = TripletModel(n_users, n_items,\n",
    "                             latent_dim=64, l2_reg=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchModel(Model):\n",
    "\n",
    "    def __init__(self, user_layer, item_layer):\n",
    "        super().__init__(name=\"MatchModel\")\n",
    "        \n",
    "        # Reuse shared weights for those layers:\n",
    "        self.user_layer = user_layer\n",
    "        self.item_layer = item_layer\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1, normalize=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        user_input = inputs[0]\n",
    "        pos_item_input = inputs[1]\n",
    "        \n",
    "        user_embedding = self.user_layer(user_input)\n",
    "        user_embedding = self.flatten(user_embedding)\n",
    "\n",
    "        pos_item_embedding = self.item_layer(pos_item_input)\n",
    "        pos_item_embedding = self.flatten(pos_item_embedding)\n",
    "    \n",
    "        pos_similarity = self.dot([user_embedding,\n",
    "                                   pos_item_embedding])\n",
    "        \n",
    "        return pos_similarity\n",
    "    \n",
    "\n",
    "match_model = MatchModel(triplet_model.user_layer,\n",
    "                         triplet_model.item_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `triplet_model` and `match_model` have as much parameters, they share both user and item embeddings. Their only difference is that the latter doesn't compute the negative similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality of Ranked Recommendations\n",
    "\n",
    "Now that we have a randomly initialized model we can start computing random recommendations. To assess their quality we do the following for each user:\n",
    "\n",
    "- compute matching scores for items (except the movies that the user has already seen in the training set),\n",
    "- compare to the positive feedback actually collected on the test set using the ROC AUC ranking metric,\n",
    "- average ROC AUC scores across users to get the average performance of the recommender model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def average_roc_auc(model, data_train, data_test):\n",
    "    \"\"\"Compute the ROC AUC for each user and average over users\"\"\"\n",
    "    max_user_id = max(data_train['user_id'].max(),\n",
    "                      data_test['user_id'].max())\n",
    "    max_item_id = max(data_train['item_id'].max(),\n",
    "                      data_test['item_id'].max())\n",
    "    user_auc_scores = []\n",
    "    for user_id in range(1, max_user_id + 1):\n",
    "        pos_item_train = data_train[data_train['user_id'] == user_id]\n",
    "        pos_item_test = data_test[data_test['user_id'] == user_id]\n",
    "        \n",
    "        # Consider all the items already seen in the training set\n",
    "        all_item_ids = np.arange(1, max_item_id + 1)\n",
    "        items_to_rank = np.setdiff1d(\n",
    "            all_item_ids, pos_item_train['item_id'].values)\n",
    "        \n",
    "        # Ground truth: return 1 for each item positively present in\n",
    "        # the test set and 0 otherwise.\n",
    "        expected = np.in1d(\n",
    "            items_to_rank, pos_item_test['item_id'].values)\n",
    "        \n",
    "        if np.sum(expected) >= 1:\n",
    "            # At least one positive test value to rank\n",
    "            repeated_user_id = np.empty_like(items_to_rank)\n",
    "            repeated_user_id.fill(user_id)\n",
    "\n",
    "            predicted = model.predict(\n",
    "                [repeated_user_id, items_to_rank], batch_size=4096)\n",
    "        \n",
    "            user_auc_scores.append(roc_auc_score(expected, predicted))\n",
    "\n",
    "    return sum(user_auc_scores) / len(user_auc_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the model should make predictions that rank the items in random order. The **ROC AUC score** is a ranking score that represents the **expected value of correctly ordering uniformly sampled pairs of recommendations**.\n",
    "\n",
    "A random (untrained) model should yield 0.50 ROC AUC on average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4978459394868747"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_roc_auc(match_model, pos_data_train, pos_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Triplet Model\n",
    "\n",
    "Let's now fit the parameters of the model by sampling triplets: for each user, select a movie in the positive feedback set of that user and randomly sample another movie to serve as negative item.\n",
    "\n",
    "Note that this sampling scheme could be improved by removing items that are marked as positive in the data to remove some label noise. In practice this does not seem to be a problem though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets(pos_data, max_item_id, random_seed=0):\n",
    "    \"\"\"Sample negatives at random\"\"\"\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    user_ids = pos_data['user_id'].values\n",
    "    pos_item_ids = pos_data['item_id'].values\n",
    "\n",
    "    neg_item_ids = rng.randint(low=1, high=max_item_id + 1,\n",
    "                               size=len(user_ids))\n",
    "\n",
    "    return [user_ids, pos_item_ids, neg_item_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the triplet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49906 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ogrisel/miniconda3/envs/pylatest/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1344/49906 [..............................] - ETA: 25s - loss: 0.3059 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ogrisel/miniconda3/envs/pylatest/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49906/49906 [==============================] - 3s 51us/sample - loss: 0.3164\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 32us/sample - loss: 0.3118\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 33us/sample - loss: 0.3128\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 34us/sample - loss: 0.3143\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 34us/sample - loss: 0.3125\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 37us/sample - loss: 0.3106\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 36us/sample - loss: 0.3132\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 46us/sample - loss: 0.3130\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 36us/sample - loss: 0.3125\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 32us/sample - loss: 0.3094\n",
      "Epoch 10/10: test ROC AUC: 0.9262\n"
     ]
    }
   ],
   "source": [
    "# we plug the identity loss and the a fake target variable ignored by\n",
    "# the model to be able to use the Keras API to train the triplet model\n",
    "fake_y = np.ones_like(pos_data_train[\"user_id\"])\n",
    "\n",
    "triplet_model.compile(loss=identity_loss, optimizer=\"adam\")\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Sample new negatives to build different triplets at each epoch\n",
    "    triplet_inputs = sample_triplets(pos_data_train, max_item_id,\n",
    "                                     random_seed=i)\n",
    "\n",
    "    # Fit the model incrementally by doing a single pass over the\n",
    "    # sampled triplets.\n",
    "    triplet_model.fit(x=triplet_inputs, y=fake_y, shuffle=True,\n",
    "                      batch_size=64, epochs=1)\n",
    "\n",
    "\n",
    "# Evaluate the convergence of the model. Ideally we should prepare a\n",
    "# validation set and compute this at each epoch but this is too slow.\n",
    "test_auc = average_roc_auc(match_model, pos_data_train, pos_data_test)\n",
    "print(\"Epoch %d/%d: test ROC AUC: %0.4f\"\n",
    "      % (i + 1, n_epochs, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Count the number of parameters in `match_model` and `triplet_model`. Which model has the largest number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MatchModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "user_embedding (Embedding)   multiple                  60416     \n",
      "_________________________________________________________________\n",
      "item_embedding (Embedding)   multiple                  107712    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dot_5 (Dot)                  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 168,128\n",
      "Trainable params: 168,128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(match_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TripletModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "user_embedding (Embedding)   multiple                  60416     \n",
      "_________________________________________________________________\n",
      "item_embedding (Embedding)   multiple                  107712    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dot_4 (Dot)                  multiple                  0         \n",
      "_________________________________________________________________\n",
      "margin_loss_2 (MarginLoss)   multiple                  0         \n",
      "=================================================================\n",
      "Total params: 168,128\n",
      "Trainable params: 168,128\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(triplet_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/triplet_parameter_count.py\n",
    "# Analysis:\n",
    "#\n",
    "# Both models have exactly the same number of parameters,\n",
    "# namely the parameters of the 2 embeddings:\n",
    "#\n",
    "# - user embedding: n_users x embedding_dim\n",
    "# - item embedding: n_items x embedding_dim\n",
    "#\n",
    "# The triplet model uses the same item embedding twice,\n",
    "# once to compute the positive similarity and the other\n",
    "# time to compute the negative similarity. However because\n",
    "# those two nodes in the computation graph share the same\n",
    "# instance of the item embedding layer, the item embedding\n",
    "# weight matrix is shared by the two branches of the\n",
    "# graph and therefore the total number of parameters for\n",
    "# each model is in both cases:\n",
    "#\n",
    "# (n_users x embedding_dim) + (n_items x embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training a Deep Matching Model on Implicit Feedback\n",
    "\n",
    "\n",
    "Instead of using hard-coded cosine similarities to predict the match of a `(user_id, item_id)` pair, we can instead specify a deep neural network based parametrisation of the similarity. The parameters of that matching model are also trained with the margin comparator loss:\n",
    "\n",
    "<img src=\"images/rec_archi_implicit_1.svg\" style=\"width: 600px;\" />\n",
    "\n",
    "\n",
    "### Exercise to complete as a home assignment:\n",
    "\n",
    "- Implement a `deep_match_model`, `deep_triplet_model` pair of models\n",
    "  for the architecture described in the schema.   The last layer of\n",
    "  the embedded Multi Layer Perceptron outputs a single scalar that\n",
    "  encodes the similarity between a user and a candidate item.\n",
    "\n",
    "- Evaluate the resulting model by computing the per-user average\n",
    "  ROC AUC score on the test feedback data.\n",
    "  \n",
    "  - Check that the AUC ROC score is close to 0.50 for a randomly\n",
    "    initialized model.\n",
    "    \n",
    "  - Check that you can reach at least 0.91 ROC AUC with this deep\n",
    "    model (you might need to adjust the hyperparameters).\n",
    "    \n",
    "    \n",
    "Hints:\n",
    "\n",
    "- it is possible to reuse the code to create embeddings from the previous model\n",
    "  definition;\n",
    "\n",
    "- the concatenation between user and the positive item embedding can be\n",
    "  obtained with the `Concatenate` layer:\n",
    "\n",
    "```py\n",
    "    concat = Concatenate()\n",
    "    \n",
    "    positive_embeddings_pair = concat([user_embedding,\n",
    "                                       positive_item_embedding])\n",
    "    negative_embeddings_pair = concat([user_embedding,\n",
    "                                       negative_item_embedding])\n",
    "```\n",
    "\n",
    "- those embedding pairs should be fed to a shared MLP instance to compute the similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.layers import Concatenate, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, n_hidden=1, hidden_size=64, dropout=0.,\n",
    "                 l2_reg=None):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "\n",
    "    \n",
    "class DeepTripletModel(Model):\n",
    "    def __init__(self, n_users, n_items, user_dim=32, item_dim=64,\n",
    "                 margin=1., n_hidden=1, hidden_size=64, dropout=0,\n",
    "                 l2_reg=None):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        \n",
    "\n",
    "class DeepMatchModel(Model):\n",
    "    def __init__(self, user_layer, item_layer, mlp):\n",
    "        super().__init__(name=\"MatchModel\")\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 3s 51us/sample - loss: 0.4641\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 45us/sample - loss: 0.3784\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 42us/sample - loss: 0.3704\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 45us/sample - loss: 0.3663\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 44us/sample - loss: 0.3617\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 43us/sample - loss: 0.3457\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 44us/sample - loss: 0.3316\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 45us/sample - loss: 0.3218\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 46us/sample - loss: 0.3118\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 47us/sample - loss: 0.3037\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 48us/sample - loss: 0.3032\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 47us/sample - loss: 0.2935\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 48us/sample - loss: 0.2871\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 50us/sample - loss: 0.2774\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 3s 57us/sample - loss: 0.2689\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 48us/sample - loss: 0.2673\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 2s 49us/sample - loss: 0.2664\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 3s 51us/sample - loss: 0.2604\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 3s 50us/sample - loss: 0.2570\n",
      "Train on 49906 samples\n",
      "49906/49906 [==============================] - 3s 51us/sample - loss: 0.2544\n",
      "Epoch 20/20: test ROC AUC: 0.9208\n"
     ]
    }
   ],
   "source": [
    "# %load solutions/deep_implicit_feedback_recsys.py\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "from tensorflow.keras.layers import Concatenate, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, n_hidden=1, hidden_size=64, dropout=0.,\n",
    "                 l2_reg=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = [Dropout(dropout)]\n",
    "\n",
    "        for _ in range(n_hidden):\n",
    "            self.layers.append(Dense(hidden_size, activation=\"relu\",\n",
    "                                     kernel_regularizer=l2_reg))\n",
    "            self.layers.append(Dropout(dropout))\n",
    "\n",
    "        self.layers.append(Dense(1, activation=\"relu\",\n",
    "                                 kernel_regularizer=l2_reg))\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer(x, training=training)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepTripletModel(Model):\n",
    "    def __init__(self, n_users, n_items, user_dim=32, item_dim=64, margin=1.,\n",
    "                 n_hidden=1, hidden_size=64, dropout=0, l2_reg=None):\n",
    "        super().__init__()\n",
    "\n",
    "        l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "\n",
    "        self.user_layer = Embedding(n_users, user_dim,\n",
    "                                    input_length=1,\n",
    "                                    input_shape=(1,),\n",
    "                                    name='user_embedding',\n",
    "                                    embeddings_regularizer=l2_reg)\n",
    "        self.item_layer = Embedding(n_items, item_dim,\n",
    "                                    input_length=1,\n",
    "                                    name=\"item_embedding\",\n",
    "                                    embeddings_regularizer=l2_reg)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "        self.mlp = MLP(n_hidden, hidden_size, dropout, l2_reg)\n",
    "\n",
    "        self.margin_loss = MarginLoss(margin)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        user_input = inputs[0]\n",
    "        pos_item_input = inputs[1]\n",
    "        neg_item_input = inputs[2]\n",
    "\n",
    "        user_embedding = self.user_layer(user_input)\n",
    "        user_embedding = self.flatten(user_embedding)\n",
    "        pos_item_embedding = self.item_layer(pos_item_input)\n",
    "        pos_item_embedding = self.flatten(pos_item_embedding)\n",
    "        neg_item_embedding = self.item_layer(neg_item_input)\n",
    "        neg_item_embedding = self.flatten(neg_item_embedding)\n",
    "\n",
    "        # Similarity computation between embeddings\n",
    "        pos_embeddings_pair = self.concat([user_embedding,\n",
    "                                           pos_item_embedding])\n",
    "        neg_embeddings_pair = self.concat([user_embedding,\n",
    "                                           neg_item_embedding])\n",
    "\n",
    "        pos_similarity = self.mlp(pos_embeddings_pair)\n",
    "        neg_similarity = self.mlp(neg_embeddings_pair)\n",
    "\n",
    "        return self.margin_loss([pos_similarity, neg_similarity])\n",
    "\n",
    "\n",
    "class DeepMatchModel(Model):\n",
    "    def __init__(self, user_layer, item_layer, mlp):\n",
    "        super().__init__(name=\"MatchModel\")\n",
    "\n",
    "        self.user_layer = user_layer\n",
    "        self.item_layer = item_layer\n",
    "        self.mlp = mlp\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_input = inputs[0]\n",
    "        pos_item_input = inputs[1]\n",
    "\n",
    "        user_embedding = self.flatten(self.user_layer(user_input))\n",
    "        pos_item_embedding = self.flatten(self.item_layer(pos_item_input))\n",
    "\n",
    "        pos_embeddings_pair = self.concat([user_embedding, pos_item_embedding])\n",
    "        pos_similarity = self.mlp(pos_embeddings_pair)\n",
    "\n",
    "        return pos_similarity\n",
    "\n",
    "\n",
    "hyper_parameters = dict(\n",
    "    user_dim=32,\n",
    "    item_dim=64,\n",
    "    n_hidden=1,\n",
    "    hidden_size=128,\n",
    "    dropout=0.1,\n",
    "    l2_reg=0.,\n",
    ")\n",
    "deep_triplet_model = DeepTripletModel(n_users, n_items,\n",
    "                                      **hyper_parameters)\n",
    "deep_match_model = DeepMatchModel(deep_triplet_model.user_layer,\n",
    "                                  deep_triplet_model.item_layer,\n",
    "                                  deep_triplet_model.mlp)\n",
    "\n",
    "deep_triplet_model.compile(loss=identity_loss, optimizer='adam')\n",
    "fake_y = np.ones_like(pos_data_train['user_id'])\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # Sample new negatives to build different triplets at each epoch\n",
    "    triplet_inputs = sample_triplets(pos_data_train, max_item_id,\n",
    "                                     random_seed=i)\n",
    "\n",
    "    # Fit the model incrementally by doing a single pass over the\n",
    "    # sampled triplets.\n",
    "    deep_triplet_model.fit(triplet_inputs, fake_y, shuffle=True,\n",
    "                           batch_size=64, epochs=1)\n",
    "\n",
    "\n",
    "# Monitor the convergence of the model\n",
    "test_auc = average_roc_auc(\n",
    "    deep_match_model, pos_data_train, pos_data_test)\n",
    "print(\"Epoch %d/%d: test ROC AUC: %0.4f\"\n",
    "      % (i + 1, n_epochs, test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Count the number of parameters in `deep_match_model` and `deep_triplet_model`. Which model has the largest number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MatchModel\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "user_embedding (Embedding)   multiple                  30208     \n",
      "_________________________________________________________________\n",
      "item_embedding (Embedding)   multiple                  107712    \n",
      "_________________________________________________________________\n",
      "mlp_1 (MLP)                  multiple                  12545     \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "concatenate_3 (Concatenate)  multiple                  0         \n",
      "=================================================================\n",
      "Total params: 150,465\n",
      "Trainable params: 150,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(deep_match_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"deep_triplet_model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "user_embedding (Embedding)   multiple                  30208     \n",
      "_________________________________________________________________\n",
      "item_embedding (Embedding)   multiple                  107712    \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "concatenate_2 (Concatenate)  multiple                  0         \n",
      "_________________________________________________________________\n",
      "mlp_1 (MLP)                  multiple                  12545     \n",
      "_________________________________________________________________\n",
      "margin_loss_4 (MarginLoss)   multiple                  0         \n",
      "=================================================================\n",
      "Total params: 150,465\n",
      "Trainable params: 150,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(deep_triplet_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/deep_triplet_parameter_count.py\n",
    "# Analysis:\n",
    "#\n",
    "# Both models have again exactly the same number of parameters,\n",
    "# namely the parameters of the 2 embeddings:\n",
    "#\n",
    "# - user embedding: n_users x user_dim\n",
    "# - item embedding: n_items x item_dim\n",
    "#\n",
    "# and the parameters of the MLP model used to compute the\n",
    "# similarity score of an (user, item) pair:\n",
    "#\n",
    "# - first hidden layer weights: (user_dim + item_dim) * hidden_size\n",
    "# - first hidden biases: hidden_size\n",
    "# - extra hidden layers weights: hidden_size * hidden_size\n",
    "# - extra hidden layers biases: hidden_size\n",
    "# - output layer weights: hidden_size * 1\n",
    "# - output layer biases: 1\n",
    "#\n",
    "# The triplet model uses the same item embedding layer\n",
    "# twice and the same MLP instance twice:\n",
    "# once to compute the positive similarity and the other\n",
    "# time to compute the negative similarity. However because\n",
    "# those two lanes in the computation graph share the same\n",
    "# instances for the item embedding layer and for the MLP,\n",
    "# their parameters are shared.\n",
    "#\n",
    "# Reminder: MLP stands for multi-layer perceptron, which is a\n",
    "# common short-hand for Feed Forward Fully Connected Neural\n",
    "# Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Possible Extensions\n",
    "\n",
    "You can implement any of the following ideas if you want to get a deeper understanding of recommender systems.\n",
    "\n",
    "\n",
    "### Leverage User and Item metadata\n",
    "\n",
    "As we did for the Explicit Feedback model, it's also possible to extend our models to take additional user and item metadata as side information when computing the match score.\n",
    "\n",
    "\n",
    "### Better Ranking Metrics\n",
    "\n",
    "In this notebook we evaluated the quality of the ranked recommendations using the ROC AUC metric. This score reflect the ability of the model to correctly rank any pair of items (sampled uniformly at random among all possible items).\n",
    "\n",
    "In practice recommender systems will only display a few recommendations to the user (typically 1 to 10). It is typically more informative to use an evaluatio metric that characterize the quality of the top ranked items and attribute less or no importance to items that are not good recommendations for a specific users. Popular ranking metrics therefore include the **Precision at k** and the **Mean Average Precision**.\n",
    "\n",
    "You can read up online about those metrics and try to implement them here.\n",
    "\n",
    "\n",
    "### Hard Negatives Sampling\n",
    "\n",
    "In this experiment we sampled negative items uniformly at random. However, after training the model for a while, it is possible that the vast majority of sampled negatives have a similarity already much lower than the positive pair and that the margin comparator loss sets the majority of the gradients to zero effectively wasting a lot of computation.\n",
    "\n",
    "Given the current state of the recsys model we could sample harder negatives with a larger likelihood to train the model better closer to its decision boundary. This strategy is implemented in the WARP loss [1].\n",
    "\n",
    "The main drawback of hard negative sampling is increasing the risk of sever overfitting if a significant fraction of the labels are noisy.\n",
    "\n",
    "\n",
    "### Factorization Machines\n",
    "\n",
    "A very popular recommender systems model is called Factorization Machines [2][3]. They two use low rank vector representations of the inputs but they do not use a cosine similarity or a neural network to model user/item compatibility.\n",
    "\n",
    "It is be possible to adapt our previous code written with Keras to replace the cosine sims / MLP with the low rank FM quadratic interactions by reading through [this gentle introduction](http://tech.adroll.com/blog/data-science/2015/08/25/factorization-machines.html).\n",
    "\n",
    "If you choose to do so, you can compare the quality of the predictions with those obtained by the [pywFM project](https://github.com/jfloff/pywFM) which provides a Python wrapper for the [official libFM C++ implementation](http://www.libfm.org/). Maciej Kula also maintains a [lighfm](http://www.libfm.org/) that implements an efficient and well documented variant in Cython and Python.\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "    [1] Wsabie: Scaling Up To Large Vocabulary Image Annotation\n",
    "    Jason Weston, Samy Bengio, Nicolas Usunier, 2011\n",
    "    https://research.google.com/pubs/pub37180.html\n",
    "\n",
    "    [2] Factorization Machines, Steffen Rendle, 2010\n",
    "    https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2010FM.pdf\n",
    "\n",
    "    [3] Factorization Machines with libFM, Steffen Rendle, 2012\n",
    "    in ACM Trans. Intell. Syst. Technol., 3(3), May.\n",
    "    http://doi.acm.org/10.1145/2168752.2168771"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
